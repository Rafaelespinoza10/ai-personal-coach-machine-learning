{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07cd624",
   "metadata": {},
   "source": [
    "# Main Model Data Integration\n",
    "## AI Personal Performance Coach - Unified Dataset Construction\n",
    "\n",
    "### Objective\n",
    "\n",
    "Build a unified dataset combining:\n",
    "- **Dataset 01**: Sleep Health & Lifestyle (sleep patterns, physical activity)\n",
    "- **Dataset 03**: Mental Health & Lifestyle Habits (work-life balance, general lifestyle)\n",
    "- **Dataset 04**: Stress Level Dataset (mental health indicators, psychological factors)\n",
    "\n",
    "### Strategy\n",
    "\n",
    "1. Normalize common variables (Age, Gender, Sleep Quality, Stress Level)\n",
    "2. Select relevant variables from Dataset 04 (exclude academic-specific variables)\n",
    "3. Perform horizontal merge using common keys\n",
    "4. Create final feature engineering\n",
    "5. Prepare unified dataset for modeling\n",
    "\n",
    "### Output\n",
    "\n",
    "- Unified dataset ready for model training\n",
    "- Features: ~30-35 variables\n",
    "- Target: Unified `stress_level` (0, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85a0dd",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858254c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d01dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MAIN MODEL DATA INTEGRATION\n",
      "============================================================\n",
      "\n",
      "üìÅ Base directory: c:\\Users\\rafae\\Desktop\\Personal_Information_App\\ai_personal_performance_coach\n",
      "üìÅ Processed data: c:\\Users\\rafae\\Desktop\\Personal_Information_App\\ai_personal_performance_coach\\datasets\\processed\n",
      "üìÅ Final dataset: c:\\Users\\rafae\\Desktop\\Personal_Information_App\\ai_personal_performance_coach\\datasets\\final\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Path configuration - CORREGIDO para notebooks/\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "PROCESSED_DATA_DIR = BASE_DIR / 'datasets' / 'processed'\n",
    "FINAL_DATA_DIR = BASE_DIR / 'datasets' / 'final'\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [FINAL_DATA_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MAIN MODEL DATA INTEGRATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ Base directory: {BASE_DIR}\")\n",
    "print(f\"üìÅ Processed data: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"üìÅ Final dataset: {FINAL_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91187e3",
   "metadata": {},
   "source": [
    "## 2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d581c54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>sleep_duration</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>physical_activity</th>\n",
       "      <th>stress_level</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>blood_pressure_diastolic</th>\n",
       "      <th>sleep_quality_category</th>\n",
       "      <th>stress_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>77</td>\n",
       "      <td>83</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>Normal</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>Normal</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>Obese</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>Poor</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>Obese</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>Poor</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>Female</td>\n",
       "      <td>59</td>\n",
       "      <td>Nurse</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>68</td>\n",
       "      <td>95</td>\n",
       "      <td>Good</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>Female</td>\n",
       "      <td>59</td>\n",
       "      <td>Nurse</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>68</td>\n",
       "      <td>95</td>\n",
       "      <td>Good</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Female</td>\n",
       "      <td>59</td>\n",
       "      <td>Nurse</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>68</td>\n",
       "      <td>95</td>\n",
       "      <td>Good</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Female</td>\n",
       "      <td>59</td>\n",
       "      <td>Nurse</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>68</td>\n",
       "      <td>95</td>\n",
       "      <td>Good</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Female</td>\n",
       "      <td>59</td>\n",
       "      <td>Nurse</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>68</td>\n",
       "      <td>95</td>\n",
       "      <td>Good</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender  Age            Occupation  sleep_duration  sleep_quality  \\\n",
       "0      Male   27     Software Engineer             6.1              6   \n",
       "1      Male   28                Doctor             6.2              6   \n",
       "2      Male   28                Doctor             6.2              6   \n",
       "3      Male   28  Sales Representative             5.9              4   \n",
       "4      Male   28  Sales Representative             5.9              4   \n",
       "..      ...  ...                   ...             ...            ...   \n",
       "369  Female   59                 Nurse             8.1              9   \n",
       "370  Female   59                 Nurse             8.0              9   \n",
       "371  Female   59                 Nurse             8.1              9   \n",
       "372  Female   59                 Nurse             8.1              9   \n",
       "373  Female   59                 Nurse             8.1              9   \n",
       "\n",
       "     physical_activity  stress_level bmi_category  heart_rate  \\\n",
       "0                   42             6   Overweight          77   \n",
       "1                   60             8       Normal          75   \n",
       "2                   60             8       Normal          75   \n",
       "3                   30             8        Obese          85   \n",
       "4                   30             8        Obese          85   \n",
       "..                 ...           ...          ...         ...   \n",
       "369                 75             3   Overweight          68   \n",
       "370                 75             3   Overweight          68   \n",
       "371                 75             3   Overweight          68   \n",
       "372                 75             3   Overweight          68   \n",
       "373                 75             3   Overweight          68   \n",
       "\n",
       "     blood_pressure_diastolic sleep_quality_category stress_category  \n",
       "0                          83               Moderate          Medium  \n",
       "1                          80               Moderate            High  \n",
       "2                          80               Moderate            High  \n",
       "3                          90                   Poor            High  \n",
       "4                          90                   Poor            High  \n",
       "..                        ...                    ...             ...  \n",
       "369                        95                   Good             Low  \n",
       "370                        95                   Good             Low  \n",
       "371                        95                   Good             Low  \n",
       "372                        95                   Good             Low  \n",
       "373                        95                   Good             Low  \n",
       "\n",
       "[374 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sleep = pd.read_csv(PROCESSED_DATA_DIR / '01_cleaned_data.csv')\n",
    "df_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3e9841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>exercise_level</th>\n",
       "      <th>diet_type</th>\n",
       "      <th>sleep_hours</th>\n",
       "      <th>stress_level</th>\n",
       "      <th>work_hours</th>\n",
       "      <th>screen_time</th>\n",
       "      <th>social_interaction</th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>stress_level_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>Male</td>\n",
       "      <td>Low</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Low</td>\n",
       "      <td>21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Vegan</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Low</td>\n",
       "      <td>48</td>\n",
       "      <td>5.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Female</td>\n",
       "      <td>Low</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>7.2</td>\n",
       "      <td>High</td>\n",
       "      <td>43</td>\n",
       "      <td>4.7</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Low</td>\n",
       "      <td>Vegan</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Low</td>\n",
       "      <td>43</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>Male</td>\n",
       "      <td>Low</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Low</td>\n",
       "      <td>35</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>57</td>\n",
       "      <td>Female</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>7.0</td>\n",
       "      <td>High</td>\n",
       "      <td>29</td>\n",
       "      <td>4.4</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>Low</td>\n",
       "      <td>Junk Food</td>\n",
       "      <td>7.1</td>\n",
       "      <td>Low</td>\n",
       "      <td>47</td>\n",
       "      <td>7.4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>9.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>42</td>\n",
       "      <td>Male</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>6.0</td>\n",
       "      <td>High</td>\n",
       "      <td>23</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>25</td>\n",
       "      <td>Male</td>\n",
       "      <td>High</td>\n",
       "      <td>Keto</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Low</td>\n",
       "      <td>51</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>28</td>\n",
       "      <td>Female</td>\n",
       "      <td>High</td>\n",
       "      <td>Vegan</td>\n",
       "      <td>6.9</td>\n",
       "      <td>High</td>\n",
       "      <td>41</td>\n",
       "      <td>6.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  gender exercise_level   diet_type  sleep_hours stress_level  \\\n",
       "0      48    Male            Low  Vegetarian          6.3          Low   \n",
       "1      31    Male       Moderate       Vegan          4.9          Low   \n",
       "2      37  Female            Low  Vegetarian          7.2         High   \n",
       "3      35    Male            Low       Vegan          7.2          Low   \n",
       "4      46    Male            Low    Balanced          7.3          Low   \n",
       "...   ...     ...            ...         ...          ...          ...   \n",
       "2995   57  Female       Moderate    Balanced          7.0         High   \n",
       "2996   27    Male            Low   Junk Food          7.1          Low   \n",
       "2997   42    Male       Moderate    Balanced          6.0         High   \n",
       "2998   25    Male           High        Keto          5.7          Low   \n",
       "2999   28  Female           High       Vegan          6.9         High   \n",
       "\n",
       "      work_hours  screen_time  social_interaction  happiness_score  \\\n",
       "0             21          4.0                 7.8              6.5   \n",
       "1             48          5.2                 8.2              6.8   \n",
       "2             43          4.7                 9.6              9.7   \n",
       "3             43          2.2                 8.2              6.6   \n",
       "4             35          3.6                 4.7              4.4   \n",
       "...          ...          ...                 ...              ...   \n",
       "2995          29          4.4                 9.7              5.9   \n",
       "2996          47          7.4                 6.3              9.9   \n",
       "2997          23          3.9                 5.2              4.1   \n",
       "2998          51          4.3                 5.9              4.1   \n",
       "2999          41          6.7                 8.3              2.2   \n",
       "\n",
       "      stress_level_numeric  \n",
       "0                        2  \n",
       "1                        2  \n",
       "2                        8  \n",
       "3                        2  \n",
       "4                        2  \n",
       "...                    ...  \n",
       "2995                     8  \n",
       "2996                     2  \n",
       "2997                     8  \n",
       "2998                     2  \n",
       "2999                     8  \n",
       "\n",
       "[3000 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lifestyle = pd.read_csv(PROCESSED_DATA_DIR / '03_cleaned_data.csv')\n",
    "df_lifestyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d13f4d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anxiety_level</th>\n",
       "      <th>self_esteem</th>\n",
       "      <th>mental_health_history</th>\n",
       "      <th>depression</th>\n",
       "      <th>headache</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>breathing_problem</th>\n",
       "      <th>noise_level</th>\n",
       "      <th>living_conditions</th>\n",
       "      <th>safety</th>\n",
       "      <th>basic_needs</th>\n",
       "      <th>academic_performance</th>\n",
       "      <th>study_load</th>\n",
       "      <th>teacher_student_relationship</th>\n",
       "      <th>future_career_concerns</th>\n",
       "      <th>social_support</th>\n",
       "      <th>peer_pressure</th>\n",
       "      <th>extracurricular_activities</th>\n",
       "      <th>bullying</th>\n",
       "      <th>stress_level</th>\n",
       "      <th>mental_health_index</th>\n",
       "      <th>protective_factors_score</th>\n",
       "      <th>stress_risk_score</th>\n",
       "      <th>environmental_quality_score</th>\n",
       "      <th>academic_stress_index</th>\n",
       "      <th>physical_symptoms_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.634921</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544974</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658730</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.510582</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521164</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.362434</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.706349</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anxiety_level  self_esteem  mental_health_history  depression  headache  \\\n",
       "0                14           20                      0          11         2   \n",
       "1                15            8                      1          15         5   \n",
       "2                12           18                      1          14         2   \n",
       "3                16           12                      1          15         4   \n",
       "4                16           28                      0           7         2   \n",
       "...             ...          ...                    ...         ...       ...   \n",
       "1095             11           17                      0          14         3   \n",
       "1096              9           12                      0           8         0   \n",
       "1097              4           26                      0           3         1   \n",
       "1098             21            0                      1          19         5   \n",
       "1099             18            6                      1          15         3   \n",
       "\n",
       "      sleep_quality  breathing_problem  noise_level  living_conditions  \\\n",
       "0                 2                  4            2                  3   \n",
       "1                 1                  4            3                  1   \n",
       "2                 2                  2            2                  2   \n",
       "3                 1                  3            4                  2   \n",
       "4                 5                  1            3                  2   \n",
       "...             ...                ...          ...                ...   \n",
       "1095              3                  2            2                  2   \n",
       "1096              0                  0            0                  1   \n",
       "1097              5                  2            2                  3   \n",
       "1098              1                  4            3                  1   \n",
       "1099              0                  3            3                  0   \n",
       "\n",
       "      safety  basic_needs  academic_performance  study_load  \\\n",
       "0          3            2                     3           2   \n",
       "1          2            2                     1           4   \n",
       "2          3            2                     2           3   \n",
       "3          2            2                     2           4   \n",
       "4          4            3                     4           3   \n",
       "...      ...          ...                   ...         ...   \n",
       "1095       2            3                     2           2   \n",
       "1096       3            4                     0           1   \n",
       "1097       4            4                     5           1   \n",
       "1098       1            1                     2           5   \n",
       "1099       4            3                     3           4   \n",
       "\n",
       "      teacher_student_relationship  future_career_concerns  social_support  \\\n",
       "0                                3                       3               2   \n",
       "1                                1                       5               1   \n",
       "2                                3                       2               2   \n",
       "3                                1                       4               1   \n",
       "4                                1                       2               1   \n",
       "...                            ...                     ...             ...   \n",
       "1095                             2                       3               3   \n",
       "1096                             1                       1               1   \n",
       "1097                             4                       1               3   \n",
       "1098                             1                       4               1   \n",
       "1099                             3                       3               1   \n",
       "\n",
       "      peer_pressure  extracurricular_activities  bullying  stress_level  \\\n",
       "0                 3                           3         2             1   \n",
       "1                 4                           5         5             2   \n",
       "2                 3                           2         2             1   \n",
       "3                 4                           4         5             2   \n",
       "4                 5                           0         5             1   \n",
       "...             ...                         ...       ...           ...   \n",
       "1095              2                           3         3             1   \n",
       "1096              3                           4         3             2   \n",
       "1097              1                           2         1             0   \n",
       "1098              4                           4         4             2   \n",
       "1099              5                           1         4             2   \n",
       "\n",
       "      mental_health_index  protective_factors_score  stress_risk_score  \\\n",
       "0                0.537037                  0.583333               0.50   \n",
       "1                0.634921                  0.250000               0.90   \n",
       "2                0.544974                  0.516667               0.50   \n",
       "3                0.658730                  0.333333               0.85   \n",
       "4                0.510582                  0.766667               0.75   \n",
       "...                   ...                       ...                ...   \n",
       "1095             0.521164                  0.641667               0.50   \n",
       "1096             0.362434                  0.183333               0.40   \n",
       "1097             0.150794                  0.966667               0.20   \n",
       "1098             0.851852                  0.233333               0.85   \n",
       "1099             0.706349                  0.283333               0.80   \n",
       "\n",
       "      environmental_quality_score  academic_stress_index  \\\n",
       "0                        0.533333               0.466667   \n",
       "1                        0.333333               0.866667   \n",
       "2                        0.466667               0.533333   \n",
       "3                        0.400000               0.733333   \n",
       "4                        0.600000               0.400000   \n",
       "...                           ...                    ...   \n",
       "1095                     0.466667               0.533333   \n",
       "1096                     0.533333               0.466667   \n",
       "1097                     0.733333               0.133333   \n",
       "1098                     0.200000               0.800000   \n",
       "1099                     0.466667               0.600000   \n",
       "\n",
       "      physical_symptoms_score  \n",
       "0                         0.6  \n",
       "1                         0.9  \n",
       "2                         0.4  \n",
       "3                         0.7  \n",
       "4                         0.3  \n",
       "...                       ...  \n",
       "1095                      0.5  \n",
       "1096                      0.0  \n",
       "1097                      0.3  \n",
       "1098                      0.9  \n",
       "1099                      0.6  \n",
       "\n",
       "[1100 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stress = pd.read_csv(PROCESSED_DATA_DIR / '04_cleaned_data.csv')\n",
    "df_stress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe023536",
   "metadata": {},
   "source": [
    "## 3. Analyze Datasets Structure\n",
    "Examine columns, data types, and key variables in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ba5984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. DATASET 01 (Sleep Health):\n",
      "   Shape: (374, 12)\n",
      "   Columns (12):\n",
      "      1. Gender\n",
      "      2. Age\n",
      "      3. Occupation\n",
      "      4. sleep_duration\n",
      "      5. sleep_quality\n",
      "      6. physical_activity\n",
      "      7. stress_level\n",
      "      8. bmi_category\n",
      "      9. heart_rate\n",
      "      10. blood_pressure_diastolic\n",
      "      11. sleep_quality_category\n",
      "      12. stress_category\n",
      "   Missing values: 0\n",
      "\n",
      "2. DATASET 03 (Mental Health Lifestyle):\n",
      "   Shape: (3000, 11)\n",
      "   Columns (11):\n",
      "      1. age\n",
      "      2. gender\n",
      "      3. exercise_level\n",
      "      4. diet_type\n",
      "      5. sleep_hours\n",
      "      6. stress_level\n",
      "      7. work_hours\n",
      "      8. screen_time\n",
      "      9. social_interaction\n",
      "      10. happiness_score\n",
      "      11. stress_level_numeric\n",
      "   Missing values: 0\n",
      "\n",
      "3. DATASET 04 (Stress Level):\n",
      "   Shape: (1100, 26)\n",
      "   Columns (26):\n",
      "      1. anxiety_level\n",
      "      2. self_esteem\n",
      "      3. mental_health_history\n",
      "      4. depression\n",
      "      5. headache\n",
      "      6. sleep_quality\n",
      "      7. breathing_problem\n",
      "      8. noise_level\n",
      "      9. living_conditions\n",
      "      10. safety\n",
      "      11. basic_needs\n",
      "      12. academic_performance\n",
      "      13. study_load\n",
      "      14. teacher_student_relationship\n",
      "      15. future_career_concerns\n",
      "      16. social_support\n",
      "      17. peer_pressure\n",
      "      18. extracurricular_activities\n",
      "      19. bullying\n",
      "      20. stress_level\n",
      "      21. mental_health_index\n",
      "      22. protective_factors_score\n",
      "      23. stress_risk_score\n",
      "      24. environmental_quality_score\n",
      "      25. academic_stress_index\n",
      "      26. physical_symptoms_score\n",
      "   Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. DATASET 01 (Sleep Health):\")\n",
    "print(f\"   Shape: {df_sleep.shape}\")\n",
    "print(f\"   Columns ({len(df_sleep.columns)}):\")\n",
    "for i, col in enumerate(df_sleep.columns, 1):\n",
    "    print(f\"      {i}. {col}\")\n",
    "print(f\"   Missing values: {df_sleep.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n2. DATASET 03 (Mental Health Lifestyle):\")\n",
    "print(f\"   Shape: {df_lifestyle.shape}\")\n",
    "print(f\"   Columns ({len(df_lifestyle.columns)}):\")\n",
    "for i, col in enumerate(df_lifestyle.columns, 1):\n",
    "    print(f\"      {i}. {col}\")\n",
    "print(f\"   Missing values: {df_lifestyle.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n3. DATASET 04 (Stress Level):\")\n",
    "print(f\"   Shape: {df_stress.shape}\")\n",
    "print(f\"   Columns ({len(df_stress.columns)}):\")\n",
    "for i, col in enumerate(df_stress.columns, 1):\n",
    "    print(f\"      {i}. {col}\")\n",
    "print(f\"   Missing values: {df_stress.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7838de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMMON VARIABLES IDENTIFICATION\n",
      "============================================================\n",
      "\n",
      "Common variables (Dataset 01 & 03): 1\n",
      "   - stress_level\n",
      "\n",
      " Common variables (All 3 datasets): 1\n",
      "   - stress_level\n"
     ]
    }
   ],
   "source": [
    "# Check common variables\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMMON VARIABLES IDENTIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "common_vars_01_03 = set(df_sleep.columns) & set(df_lifestyle.columns)\n",
    "common_vars_all = set(df_sleep.columns) & set(df_lifestyle.columns) & set(df_stress.columns)\n",
    "\n",
    "print(f\"\\nCommon variables (Dataset 01 & 03): {len(common_vars_01_03)}\")\n",
    "for var in sorted(common_vars_01_03):\n",
    "    print(f\"   - {var}\")\n",
    "\n",
    "print(f\"\\n Common variables (All 3 datasets): {len(common_vars_all)}\")\n",
    "for var in sorted(common_vars_all):\n",
    "    print(f\"   - {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71a7c9",
   "metadata": {},
   "source": [
    "## 4. Normalize common variables\n",
    "Standardize variable names, scales, and formats across datasets for successful merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a841749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_norm = df_sleep.copy()\n",
    "df_lifestyle_norm = df_lifestyle.copy()\n",
    "df_stress_norm = df_stress.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7613cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Age normalization:\n",
      "    Dataset 01: 'Age' ‚Üí 'age'\n",
      "   Dataset 03: already 'age'\n",
      "    Dataset 04: 'age' created synthetically (18-30, student population)\n",
      "\n",
      "   Age ranges:\n",
      "      Dataset 01: 27-59\n",
      "      Dataset 03: 18-64\n",
      "      Dataset 04: 18-29\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Age normalization:\")\n",
    "if 'Age' in df_sleep_norm.columns:\n",
    "    df_sleep_norm = df_sleep_norm.rename(columns={'Age': 'age'})\n",
    "    print(\"    Dataset 01: 'Age' ‚Üí 'age'\")\n",
    "\n",
    "if 'age' in df_lifestyle_norm.columns:\n",
    "    print(\"   Dataset 03: already 'age'\")\n",
    "\n",
    "\n",
    "if 'age' not in df_stress_norm.columns:\n",
    "    # Create synthetic age (random between 18-30, representative of student population)\n",
    "    np.random.seed(42)\n",
    "    df_stress_norm['age'] = np.random.randint(18, 30, size=len(df_stress_norm))\n",
    "    print(\"    Dataset 04: 'age' created synthetically (18-30, student population)\")\n",
    "\n",
    "# Verify age ranges\n",
    "print(f\"\\n   Age ranges:\")\n",
    "if 'age' in df_sleep_norm.columns:\n",
    "    print(f\"      Dataset 01: {df_sleep_norm['age'].min()}-{df_sleep_norm['age'].max()}\")\n",
    "if 'age' in df_lifestyle_norm.columns:\n",
    "    print(f\"      Dataset 03: {df_lifestyle_norm['age'].min()}-{df_lifestyle_norm['age'].max()}\")\n",
    "if 'age' in df_stress_norm.columns:\n",
    "    print(f\"      Dataset 04: {df_stress_norm['age'].min()}-{df_stress_norm['age'].max()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfb2159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Gender normalization:\n",
      "   Dataset 01: 'Gender' ‚Üí 'gender'\n",
      "   Dataset 03: already 'gender'\n",
      "    Dataset 04: 'gender' created synthetically\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. GENDER NORMALIZATION\n",
    "# ============================================\n",
    "print(\"\\n2. Gender normalization:\")\n",
    "\n",
    "# Dataset 01: 'Gender' -> 'gender'\n",
    "if 'Gender' in df_sleep_norm.columns:\n",
    "    df_sleep_norm = df_sleep_norm.rename(columns={'Gender': 'gender'})\n",
    "    print(\"   Dataset 01: 'Gender' ‚Üí 'gender'\")\n",
    "\n",
    "# Dataset 03: already 'gender'\n",
    "if 'gender' in df_lifestyle_norm.columns:\n",
    "    print(\"   Dataset 03: already 'gender'\")\n",
    "\n",
    "# Dataset 04: check if exists\n",
    "if 'gender' not in df_stress_norm.columns:\n",
    "    # Create synthetic gender (balanced distribution)\n",
    "    np.random.seed(42)\n",
    "    df_stress_norm['gender'] = np.random.choice(['Male', 'Female'], size=len(df_stress_norm), p=[0.5, 0.5])\n",
    "    print(\"    Dataset 04: 'gender' created synthetically\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e25b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Gender distributions:\n",
      "      Dataset 01: {'Male': 189, 'Female': 185}\n",
      "      Dataset 03: {'Female': 1024, 'Other': 996, 'Male': 980}\n",
      "      Dataset 04: {'Male': 550, 'Female': 550}\n"
     ]
    }
   ],
   "source": [
    "gender_mapping = {'male': 'Male', 'female': 'Female', 'M': 'Male', 'F': 'Female', 'Other': 'Other'}\n",
    "for df in [df_sleep_norm, df_lifestyle_norm, df_stress_norm]:\n",
    "    if 'gender' in df.columns:\n",
    "        df['gender'] = df['gender'].str.title().replace(gender_mapping)\n",
    "\n",
    "print(f\"\\n   Gender distributions:\")\n",
    "for name, df in [('Dataset 01', df_sleep_norm), ('Dataset 03', df_lifestyle_norm), ('Dataset 04', df_stress_norm)]:\n",
    "    if 'gender' in df.columns:\n",
    "        print(f\"      {name}: {df['gender'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c65f1e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Sleep quality normalization:\n",
      "   Dataset 01 - sleep_quality range: 4-9\n",
      "   Dataset 03 - sleep_hours range: 1.4-11.3\n",
      "   Dataset 04 - sleep_quality range: 0-5\n",
      "    Created 'sleep_quality_norm' in all datasets (0-1 scale)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. SLEEP QUALITY NORMALIZATION\n",
    "# ============================================\n",
    "print(\"\\n3. Sleep quality normalization:\")\n",
    "\n",
    "# Dataset 01: 'sleep_quality' (numeric scale)\n",
    "if 'sleep_quality' in df_sleep_norm.columns:\n",
    "    print(f\"   Dataset 01 - sleep_quality range: {df_sleep_norm['sleep_quality'].min()}-{df_sleep_norm['sleep_quality'].max()}\")\n",
    "    # Normalize to 0-1 scale\n",
    "    sleep_max_01 = df_sleep_norm['sleep_quality'].max()\n",
    "    if sleep_max_01 > 0:\n",
    "        df_sleep_norm['sleep_quality_norm'] = df_sleep_norm['sleep_quality'] / sleep_max_01\n",
    "    else:\n",
    "        df_sleep_norm['sleep_quality_norm'] = df_sleep_norm['sleep_quality']\n",
    "\n",
    "# Dataset 03: 'sleep_hours' (continuous hours)\n",
    "if 'sleep_hours' in df_lifestyle_norm.columns:\n",
    "    print(f\"   Dataset 03 - sleep_hours range: {df_lifestyle_norm['sleep_hours'].min()}-{df_lifestyle_norm['sleep_hours'].max()}\")\n",
    "    # Normalize to 0-1 scale (assuming 0-12 hours range)\n",
    "    sleep_max_03 = max(df_lifestyle_norm['sleep_hours'].max(), 12)\n",
    "    df_lifestyle_norm['sleep_quality_norm'] = df_lifestyle_norm['sleep_hours'] / sleep_max_03\n",
    "    # Also keep original for reference\n",
    "    df_lifestyle_norm['sleep_hours_original'] = df_lifestyle_norm['sleep_hours']\n",
    "\n",
    "# Dataset 04: 'sleep_quality' (0-5 scale based on EDA)\n",
    "if 'sleep_quality' in df_stress_norm.columns:\n",
    "    print(f\"   Dataset 04 - sleep_quality range: {df_stress_norm['sleep_quality'].min()}-{df_stress_norm['sleep_quality'].max()}\")\n",
    "    # Normalize to 0-1 scale\n",
    "    sleep_max_04 = df_stress_norm['sleep_quality'].max()\n",
    "    if sleep_max_04 > 0:\n",
    "        df_stress_norm['sleep_quality_norm'] = df_stress_norm['sleep_quality'] / sleep_max_04\n",
    "    else:\n",
    "        df_stress_norm['sleep_quality_norm'] = df_stress_norm['sleep_quality']\n",
    "\n",
    "print(\"    Created 'sleep_quality_norm' in all datasets (0-1 scale)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5246629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. STRESS LEVEL NORMALIZATION\n",
    "# ============================================\n",
    "\n",
    "# Function to normalize stress levels to 0, 1, 2 scale\n",
    "def normalize_stress_level(value):\n",
    "    \"\"\"Normalize stress level to 0 (Low), 1 (Medium), 2 (High)\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    # If already numeric 0-2\n",
    "    if isinstance(value, (int, float)):\n",
    "        if 0 <= value <= 2:\n",
    "            return int(value)\n",
    "        # If 0-10 scale, map to 0-2\n",
    "        elif 0 <= value <= 10:\n",
    "            if value <= 3:\n",
    "                return 0  # Low\n",
    "            elif value <= 7:\n",
    "                return 1  # Medium\n",
    "            else:\n",
    "                return 2  # High\n",
    "    \n",
    "    # If categorical\n",
    "    if isinstance(value, str):\n",
    "        value_lower = value.lower()\n",
    "        if 'low' in value_lower or value == '0':\n",
    "            return 0\n",
    "        elif 'medium' in value_lower or 'moderate' in value_lower or value == '1':\n",
    "            return 1\n",
    "        elif 'high' in value_lower or value == '2':\n",
    "            return 2\n",
    "    \n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8107d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dataset 01 - stress_level unique values: [6 8 7 4 3 5]\n",
      "   Dataset 01 - stress_level_norm distribution: {0: 71, 1: 233, 2: 70}\n",
      "   Dataset 03 - stress_level unique values: ['Low' 'High' 'Moderate']\n",
      "   Dataset 03 - stress_level_norm distribution: {0: 1008, 1: 990, 2: 1002}\n",
      "   Dataset 04 - stress_level unique values: [1 2 0]\n",
      "   Dataset 04 - stress_level_norm distribution: {0: 373, 1: 358, 2: 369}\n"
     ]
    }
   ],
   "source": [
    "# Normalize stress levels\n",
    "if 'stress_level' in df_sleep_norm.columns:\n",
    "    df_sleep_norm['stress_level_norm'] = df_sleep_norm['stress_level'].apply(normalize_stress_level)\n",
    "    print(f\"   Dataset 01 - stress_level unique values: {df_sleep_norm['stress_level'].unique()}\")\n",
    "    print(f\"   Dataset 01 - stress_level_norm distribution: {df_sleep_norm['stress_level_norm'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "if 'stress_level' in df_lifestyle_norm.columns:\n",
    "    df_lifestyle_norm['stress_level_norm'] = df_lifestyle_norm['stress_level'].apply(normalize_stress_level)\n",
    "    print(f\"   Dataset 03 - stress_level unique values: {df_lifestyle_norm['stress_level'].unique()}\")\n",
    "    print(f\"   Dataset 03 - stress_level_norm distribution: {df_lifestyle_norm['stress_level_norm'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "if 'stress_level' in df_stress_norm.columns:\n",
    "    df_stress_norm['stress_level_norm'] = df_stress_norm['stress_level'].apply(normalize_stress_level)\n",
    "    print(f\"   Dataset 04 - stress_level unique values: {df_stress_norm['stress_level'].unique()}\")\n",
    "    print(f\"   Dataset 04 - stress_level_norm distribution: {df_stress_norm['stress_level_norm'].value_counts().sort_index().to_dict()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e00fc8",
   "metadata": {},
   "source": [
    "## 5. Select variables for dataset 04\n",
    "select only relevant variables from Dataset 04 (exclude academic-specific variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b259387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anxiety_level',\n",
       " 'self_esteem',\n",
       " 'depression',\n",
       " 'mental_health_history',\n",
       " 'headache',\n",
       " 'breathing_problem',\n",
       " 'sleep_quality',\n",
       " 'sleep_quality_norm',\n",
       " 'noise_level',\n",
       " 'living_conditions',\n",
       " 'safety',\n",
       " 'basic_needs',\n",
       " 'mental_health_index',\n",
       " 'protective_factors_score',\n",
       " 'stress_risk_score',\n",
       " 'environmental_quality_score',\n",
       " 'physical_symptoms_score',\n",
       " 'stress_level_norm',\n",
       " 'age',\n",
       " 'gender']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_include_dataset_dataset_04 = [ \n",
    "      # Mental health indicators\n",
    "    'anxiety_level',\n",
    "    'self_esteem',\n",
    "    'depression',\n",
    "    'mental_health_history',\n",
    "    \n",
    "    # Physical symptoms (general)\n",
    "    'headache',\n",
    "    'breathing_problem',\n",
    "    \n",
    "    # Sleep (already normalized)\n",
    "    'sleep_quality',\n",
    "    'sleep_quality_norm',\n",
    "    \n",
    "    # Environmental factors (general)\n",
    "    'noise_level',\n",
    "    'living_conditions',\n",
    "    'safety',\n",
    "    'basic_needs',\n",
    "    \n",
    "    # Engineered features (from previous feature engineering)\n",
    "    'mental_health_index',\n",
    "    'protective_factors_score',\n",
    "    'stress_risk_score',\n",
    "    'environmental_quality_score',\n",
    "    'physical_symptoms_score',\n",
    "    \n",
    "    # Normalized variables\n",
    "    'stress_level_norm',\n",
    "    'age',\n",
    "    'gender'\n",
    "]\n",
    "features_to_include_dataset_dataset_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ee13da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study_load',\n",
       " 'teacher_student_relationship',\n",
       " 'academic_performance',\n",
       " 'academic_stress_index',\n",
       " 'peer_pressure',\n",
       " 'future_career_concerns',\n",
       " 'extracurricular_activities',\n",
       " 'bullying']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_exclude_dataset_04 = [\n",
    "    'study_load',\n",
    "    'teacher_student_relationship',\n",
    "    'academic_performance',\n",
    "    'academic_stress_index',  # Academic-specific index\n",
    "    'peer_pressure',  # Can be general, but primarily academic context\n",
    "    'future_career_concerns',  # Academic-specific\n",
    "    'extracurricular_activities',  # Academic-specific\n",
    "    'bullying',  # Context-dependent, but may be academic\n",
    "]\n",
    "features_to_exclude_dataset_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e35aa883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually, let's keep social_support as it's general\n",
    "features_to_include_dataset_dataset_04.append(\"social_support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fc65241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Variables to INCLUDE from Dataset 04: 21\n",
      "    anxiety_level\n",
      "    self_esteem\n",
      "    depression\n",
      "    mental_health_history\n",
      "    headache\n",
      "    breathing_problem\n",
      "    sleep_quality\n",
      "    sleep_quality_norm\n",
      "    noise_level\n",
      "    living_conditions\n",
      "    safety\n",
      "    basic_needs\n",
      "    mental_health_index\n",
      "    protective_factors_score\n",
      "    stress_risk_score\n",
      "    environmental_quality_score\n",
      "    physical_symptoms_score\n",
      "    stress_level_norm\n",
      "    age\n",
      "    gender\n",
      "    social_support\n",
      "\n",
      " Variables to EXCLUDE from Dataset 04: 21\n",
      "    study_load\n",
      "    teacher_student_relationship\n",
      "    academic_performance\n",
      "    academic_stress_index\n",
      "    peer_pressure\n",
      "    future_career_concerns\n",
      "    extracurricular_activities\n",
      "    bullying\n",
      "\n",
      " Selected 21 variables from Dataset 04\n",
      "   Final shape: (1100, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anxiety_level</th>\n",
       "      <th>self_esteem</th>\n",
       "      <th>depression</th>\n",
       "      <th>mental_health_history</th>\n",
       "      <th>headache</th>\n",
       "      <th>breathing_problem</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>sleep_quality_norm</th>\n",
       "      <th>noise_level</th>\n",
       "      <th>living_conditions</th>\n",
       "      <th>safety</th>\n",
       "      <th>basic_needs</th>\n",
       "      <th>mental_health_index</th>\n",
       "      <th>protective_factors_score</th>\n",
       "      <th>stress_risk_score</th>\n",
       "      <th>environmental_quality_score</th>\n",
       "      <th>physical_symptoms_score</th>\n",
       "      <th>stress_level_norm</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>social_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.634921</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.544974</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658730</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.510582</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.521164</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.362434</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.706349</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anxiety_level  self_esteem  depression  mental_health_history  headache  \\\n",
       "0                14           20          11                      0         2   \n",
       "1                15            8          15                      1         5   \n",
       "2                12           18          14                      1         2   \n",
       "3                16           12          15                      1         4   \n",
       "4                16           28           7                      0         2   \n",
       "...             ...          ...         ...                    ...       ...   \n",
       "1095             11           17          14                      0         3   \n",
       "1096              9           12           8                      0         0   \n",
       "1097              4           26           3                      0         1   \n",
       "1098             21            0          19                      1         5   \n",
       "1099             18            6          15                      1         3   \n",
       "\n",
       "      breathing_problem  sleep_quality  sleep_quality_norm  noise_level  \\\n",
       "0                     4              2                 0.4            2   \n",
       "1                     4              1                 0.2            3   \n",
       "2                     2              2                 0.4            2   \n",
       "3                     3              1                 0.2            4   \n",
       "4                     1              5                 1.0            3   \n",
       "...                 ...            ...                 ...          ...   \n",
       "1095                  2              3                 0.6            2   \n",
       "1096                  0              0                 0.0            0   \n",
       "1097                  2              5                 1.0            2   \n",
       "1098                  4              1                 0.2            3   \n",
       "1099                  3              0                 0.0            3   \n",
       "\n",
       "      living_conditions  safety  basic_needs  mental_health_index  \\\n",
       "0                     3       3            2             0.537037   \n",
       "1                     1       2            2             0.634921   \n",
       "2                     2       3            2             0.544974   \n",
       "3                     2       2            2             0.658730   \n",
       "4                     2       4            3             0.510582   \n",
       "...                 ...     ...          ...                  ...   \n",
       "1095                  2       2            3             0.521164   \n",
       "1096                  1       3            4             0.362434   \n",
       "1097                  3       4            4             0.150794   \n",
       "1098                  1       1            1             0.851852   \n",
       "1099                  0       4            3             0.706349   \n",
       "\n",
       "      protective_factors_score  stress_risk_score  \\\n",
       "0                     0.583333               0.50   \n",
       "1                     0.250000               0.90   \n",
       "2                     0.516667               0.50   \n",
       "3                     0.333333               0.85   \n",
       "4                     0.766667               0.75   \n",
       "...                        ...                ...   \n",
       "1095                  0.641667               0.50   \n",
       "1096                  0.183333               0.40   \n",
       "1097                  0.966667               0.20   \n",
       "1098                  0.233333               0.85   \n",
       "1099                  0.283333               0.80   \n",
       "\n",
       "      environmental_quality_score  physical_symptoms_score  stress_level_norm  \\\n",
       "0                        0.533333                      0.6                  1   \n",
       "1                        0.333333                      0.9                  2   \n",
       "2                        0.466667                      0.4                  1   \n",
       "3                        0.400000                      0.7                  2   \n",
       "4                        0.600000                      0.3                  1   \n",
       "...                           ...                      ...                ...   \n",
       "1095                     0.466667                      0.5                  1   \n",
       "1096                     0.533333                      0.0                  2   \n",
       "1097                     0.733333                      0.3                  0   \n",
       "1098                     0.200000                      0.9                  2   \n",
       "1099                     0.466667                      0.6                  2   \n",
       "\n",
       "      age  gender  social_support  \n",
       "0      24    Male               2  \n",
       "1      21  Female               1  \n",
       "2      28  Female               2  \n",
       "3      25  Female               1  \n",
       "4      22    Male               1  \n",
       "...   ...     ...             ...  \n",
       "1095   20    Male               3  \n",
       "1096   22    Male               1  \n",
       "1097   22    Male               3  \n",
       "1098   22  Female               1  \n",
       "1099   20    Male               1  \n",
       "\n",
       "[1100 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n Variables to INCLUDE from Dataset 04: {len(features_to_include_dataset_dataset_04)}\")\n",
    "for var in features_to_include_dataset_dataset_04:\n",
    "    if var in df_stress_norm.columns:\n",
    "        print(f\"    {var}\")\n",
    "    else:\n",
    "        print(f\"   {var} (not found)\")\n",
    "\n",
    "print(f\"\\n Variables to EXCLUDE from Dataset 04: {len(features_to_include_dataset_dataset_04)}\")\n",
    "for var in features_to_exclude_dataset_04:\n",
    "    print(f\"    {var}\")\n",
    "\n",
    "# Filter Dataset 04\n",
    "vars_available_04 = [var for var in features_to_include_dataset_dataset_04 if var in df_stress_norm.columns]\n",
    "df_stress_selected = df_stress_norm[vars_available_04].copy()\n",
    "\n",
    "print(f\"\\n Selected {len(vars_available_04)} variables from Dataset 04\")\n",
    "print(f\"   Final shape: {df_stress_selected.shape}\")\n",
    "\n",
    "df_stress_selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ad782",
   "metadata": {},
   "source": [
    "## 6. Pre-Merge Review and Corrections\n",
    "\n",
    "Before performing the merge, we need to:\n",
    "1. **Remove redundant variables** (duplicates after normalization)\n",
    "2. **Verify data consistency** across datasets\n",
    "3. **Resolve naming conflicts** between similar variables\n",
    "4. **Ensure proper data types** for merging\n",
    "5. **Document all transformations** for reproducibility\n",
    "\n",
    "### Issues Identified:\n",
    "\n",
    "- **Dataset 03**: `sleep_hours` (continuous) is conceptually different from `sleep_quality_norm` (normalized quality score). We should keep both as they capture different aspects.\n",
    "- **Dataset 04**: Has both `sleep_quality` (0-5) and `sleep_quality_norm` (0-1). Remove original to avoid redundancy.\n",
    "- **Dataset 03**: Has both `stress_level_numeric` and `stress_level_norm`. Remove `stress_level_numeric` to avoid redundancy.\n",
    "- **Variable naming**: `physical_activity` (Dataset 01) vs `exercise_level` (Dataset 03) are similar but have different scales - keep both as separate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "714886fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "1. COMPLETE VARIABLE INVENTORY PER DATASET\n",
      "======================================================================\n",
      "\n",
      " DATASET 01 (Sleep Health) - After Normalization:\n",
      "   Shape: 374 rows √ó 14 columns\n",
      "   Total columns: 14\n",
      "\n",
      "   Column List:\n",
      "       1. gender                         [object    ] Correct 0 nulls\n",
      "       2. age                            [int64     ] Correct 0 nulls\n",
      "       3. Occupation                     [object    ] Correct 0 nulls\n",
      "       4. sleep_duration                 [float64   ] Correct 0 nulls\n",
      "       5. sleep_quality                  [int64     ] Correct 0 nulls\n",
      "       6. physical_activity              [int64     ] Correct 0 nulls\n",
      "       7. stress_level                   [int64     ] Correct 0 nulls\n",
      "       8. bmi_category                   [object    ] Correct 0 nulls\n",
      "       9. heart_rate                     [int64     ] Correct 0 nulls\n",
      "      10. blood_pressure_diastolic       [int64     ] Correct 0 nulls\n",
      "      11. sleep_quality_category         [object    ] Correct 0 nulls\n",
      "      12. stress_category                [object    ] Correct 0 nulls\n",
      "      13. sleep_quality_norm             [float64   ] Correct 0 nulls\n",
      "      14. stress_level_norm              [int64     ] Correct 0 nulls\n"
     ]
    }
   ],
   "source": [
    "# 1. COMPLETE VARIABLE INVENTORY\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. COMPLETE VARIABLE INVENTORY PER DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n DATASET 01 (Sleep Health) - After Normalization:\")\n",
    "print(f\"   Shape: {df_sleep_norm.shape[0]} rows √ó {df_sleep_norm.shape[1]} columns\")\n",
    "print(f\"   Total columns: {len(df_sleep_norm.columns)}\")\n",
    "print(\"\\n   Column List:\")\n",
    "for i, col in enumerate(df_sleep_norm.columns, 1):\n",
    "    dtype = str(df_sleep_norm[col].dtype)  # Convert to string\n",
    "    null_count = df_sleep_norm[col].isnull().sum()\n",
    "    status = \"Dangerous\" if null_count > 0 else \"Correct\"\n",
    "    print(f\"      {i:2d}. {col:30s} [{dtype:10s}] {status} {null_count} nulls\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a36fc1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATASET 03 (Mental Health Lifestyle) - After Normalization:\n",
      "   Shape: 3000 rows √ó 14 columns\n",
      "   Total columns: 14\n",
      "\n",
      "   Column List:\n",
      "       1. age                            [int64     ] Correct 0 nulls\n",
      "       2. gender                         [object    ] Correct 0 nulls\n",
      "       3. exercise_level                 [object    ] Correct 0 nulls\n",
      "       4. diet_type                      [object    ] Correct 0 nulls\n",
      "       5. sleep_hours                    [float64   ] Correct 0 nulls\n",
      "       6. stress_level                   [object    ] Correct 0 nulls\n",
      "       7. work_hours                     [int64     ] Correct 0 nulls\n",
      "       8. screen_time                    [float64   ] Correct 0 nulls\n",
      "       9. social_interaction             [float64   ] Correct 0 nulls\n",
      "      10. happiness_score                [float64   ] Correct 0 nulls\n",
      "      11. stress_level_numeric           [int64     ] Correct 0 nulls\n",
      "      12. sleep_quality_norm             [float64   ] Correct 0 nulls\n",
      "      13. sleep_hours_original           [float64   ] Correct 0 nulls\n",
      "      14. stress_level_norm              [int64     ] Correct 0 nulls\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n DATASET 03 (Mental Health Lifestyle) - After Normalization:\")\n",
    "print(f\"   Shape: {df_lifestyle_norm.shape[0]} rows √ó {df_lifestyle_norm.shape[1]} columns\")\n",
    "print(f\"   Total columns: {len(df_lifestyle_norm.columns)}\")\n",
    "print(\"\\n   Column List:\")\n",
    "for i, col in enumerate(df_lifestyle_norm.columns, 1):\n",
    "    dtype = str(df_lifestyle_norm[col].dtype)  # Convert to string\n",
    "    null_count = df_lifestyle_norm[col].isnull().sum()\n",
    "    status = \"Dangerous\" if null_count > 0 else \"Correct\"\n",
    "    print(f\"      {i:2d}. {col:30s} [{dtype:10s}] {status} {null_count} nulls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ff718f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATASET 04 (Stress Level) - After Selection:\n",
      "   Shape: 1100 rows √ó 21 columns\n",
      "   Total columns: 21\n",
      "\n",
      "   Column List:\n",
      "       1. anxiety_level                  [int64     ] Correct 0 nulls\n",
      "       2. self_esteem                    [int64     ] Correct 0 nulls\n",
      "       3. depression                     [int64     ] Correct 0 nulls\n",
      "       4. mental_health_history          [int64     ] Correct 0 nulls\n",
      "       5. headache                       [int64     ] Correct 0 nulls\n",
      "       6. breathing_problem              [int64     ] Correct 0 nulls\n",
      "       7. sleep_quality                  [int64     ] Correct 0 nulls\n",
      "       8. sleep_quality_norm             [float64   ] Correct 0 nulls\n",
      "       9. noise_level                    [int64     ] Correct 0 nulls\n",
      "      10. living_conditions              [int64     ] Correct 0 nulls\n",
      "      11. safety                         [int64     ] Correct 0 nulls\n",
      "      12. basic_needs                    [int64     ] Correct 0 nulls\n",
      "      13. mental_health_index            [float64   ] Correct 0 nulls\n",
      "      14. protective_factors_score       [float64   ] Correct 0 nulls\n",
      "      15. stress_risk_score              [float64   ] Correct 0 nulls\n",
      "      16. environmental_quality_score    [float64   ] Correct 0 nulls\n",
      "      17. physical_symptoms_score        [float64   ] Correct 0 nulls\n",
      "      18. stress_level_norm              [int64     ] Correct 0 nulls\n",
      "      19. age                            [int32     ] Correct 0 nulls\n",
      "      20. gender                         [object    ] Correct 0 nulls\n",
      "      21. social_support                 [int64     ] Correct 0 nulls\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n DATASET 04 (Stress Level) - After Selection:\")\n",
    "print(f\"   Shape: {df_stress_selected.shape[0]} rows √ó {df_stress_selected.shape[1]} columns\")\n",
    "print(f\"   Total columns: {len(df_stress_selected.columns)}\")\n",
    "print(\"\\n   Column List:\")\n",
    "for i, col in enumerate(df_stress_selected.columns, 1):\n",
    "    dtype = str(df_stress_selected[col].dtype)  # Convert to string\n",
    "    null_count = df_stress_selected[col].isnull().sum()\n",
    "    status = \"Dangerous\" if null_count > 0 else \"Correct\"\n",
    "    print(f\"      {i:2d}. {col:30s} [{dtype:10s}] {status} {null_count} nulls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69e66d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "2. REDUNDANT VARIABLES DETAILED ANALYSIS\n",
      "======================================================================\n",
      "\n",
      " DATASET 01:\n",
      "\n",
      "    Potential redundancy: sleep_quality_pair\n",
      "      Original: sleep_quality (4-9)\n",
      "      Normalized: sleep_quality_norm (0.444-1.000)\n",
      "      Decision: KEEP BOTH - Different scales provide different information\n",
      "\n",
      "    Potential redundancy: stress_level_pair\n",
      "      Original: stress_level ([np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)])\n",
      "      Normalized: stress_level_norm ([np.int64(0), np.int64(1), np.int64(2)])\n",
      "      Decision: KEEP BOTH - Original for reference, normalized for model\n"
     ]
    }
   ],
   "source": [
    "# 2. REDUNDANT VARIABLES DETAILED ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. REDUNDANT VARIABLES DETAILED ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n DATASET 01:\")\n",
    "redundancies_01 = {}\n",
    "if 'sleep_quality' in df_sleep_norm.columns and 'sleep_quality_norm' in df_sleep_norm.columns:\n",
    "    sleep_qual_range = f\"{df_sleep_norm['sleep_quality'].min()}-{df_sleep_norm['sleep_quality'].max()}\"\n",
    "    sleep_qual_norm_range = f\"{df_sleep_norm['sleep_quality_norm'].min():.3f}-{df_sleep_norm['sleep_quality_norm'].max():.3f}\"\n",
    "    redundancies_01['sleep_quality_pair'] = {\n",
    "        'original': f\"sleep_quality ({sleep_qual_range})\",\n",
    "        'normalized': f\"sleep_quality_norm ({sleep_qual_norm_range})\",\n",
    "        'decision': 'KEEP BOTH - Different scales provide different information'\n",
    "    }\n",
    "\n",
    "if 'stress_level' in df_sleep_norm.columns and 'stress_level_norm' in df_sleep_norm.columns:\n",
    "    stress_orig_vals = sorted(df_sleep_norm['stress_level'].unique())\n",
    "    stress_norm_vals = sorted(df_sleep_norm['stress_level_norm'].dropna().unique())\n",
    "    redundancies_01['stress_level_pair'] = {\n",
    "        'original': f\"stress_level ({stress_orig_vals})\",\n",
    "        'normalized': f\"stress_level_norm ({stress_norm_vals})\",\n",
    "        'decision': 'KEEP BOTH - Original for reference, normalized for model'\n",
    "    }\n",
    "\n",
    "if redundancies_01:\n",
    "    for key, info in redundancies_01.items():\n",
    "        print(f\"\\n    Potential redundancy: {key}\")\n",
    "        print(f\"      Original: {info['original']}\")\n",
    "        print(f\"      Normalized: {info['normalized']}\")\n",
    "        print(f\"      Decision: {info['decision']}\")\n",
    "else:\n",
    "    print(\"  No redundancies found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e6b7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATASET 03:\n",
      "\n",
      "    Potential redundancy: sleep_pair\n",
      "      Original: sleep_hours (1.4-11.3 hours)\n",
      "      Derived: sleep_quality_norm (0.117-0.942)\n",
      "      Decision: KEEP BOTH - Conceptually different (quantity vs quality)\n",
      "\n",
      "    Potential redundancy: stress_level_triple\n",
      "      Categorical: stress_level (['High', 'Low', 'Moderate'])\n",
      "      Numeric: stress_level_numeric ([np.int64(2), np.int64(5), np.int64(8)])\n",
      "      Normalized: stress_level_norm ([np.int64(0), np.int64(1), np.int64(2)])\n",
      "      Are numeric and norm identical? False\n",
      "      Decision: REVIEW - May not be identical\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n DATASET 03:\")\n",
    "redundancies_03 = {}\n",
    "if 'sleep_hours' in df_lifestyle_norm.columns and 'sleep_quality_norm' in df_lifestyle_norm.columns:\n",
    "    sleep_hours_range = f\"{df_lifestyle_norm['sleep_hours'].min():.1f}-{df_lifestyle_norm['sleep_hours'].max():.1f}\"\n",
    "    sleep_qual_norm_range = f\"{df_lifestyle_norm['sleep_quality_norm'].min():.3f}-{df_lifestyle_norm['sleep_quality_norm'].max():.3f}\"\n",
    "    redundancies_03['sleep_pair'] = {\n",
    "        'original': f\"sleep_hours ({sleep_hours_range} hours)\",\n",
    "        'derived': f\"sleep_quality_norm ({sleep_qual_norm_range})\",\n",
    "        'decision': 'KEEP BOTH - Conceptually different (quantity vs quality)'\n",
    "    }\n",
    "\n",
    "if 'stress_level' in df_lifestyle_norm.columns and 'stress_level_numeric' in df_lifestyle_norm.columns and 'stress_level_norm' in df_lifestyle_norm.columns:\n",
    "    stress_cat_vals = sorted(df_lifestyle_norm['stress_level'].unique())\n",
    "    stress_num_vals = sorted(df_lifestyle_norm['stress_level_numeric'].unique())\n",
    "    stress_norm_vals = sorted(df_lifestyle_norm['stress_level_norm'].dropna().unique())\n",
    "    \n",
    "    # Verify if stress_level_numeric and stress_level_norm are identical\n",
    "    if len(df_lifestyle_norm) == len(df_lifestyle_norm[df_lifestyle_norm['stress_level_numeric'].notna()]):\n",
    "        comparison = (df_lifestyle_norm['stress_level_numeric'] == df_lifestyle_norm['stress_level_norm']).all()\n",
    "        are_identical = comparison if isinstance(comparison, bool) else comparison.sum() == len(df_lifestyle_norm)\n",
    "    else:\n",
    "        are_identical = False\n",
    "    \n",
    "    redundancies_03['stress_level_triple'] = {\n",
    "        'categorical': f\"stress_level ({stress_cat_vals})\",\n",
    "        'numeric': f\"stress_level_numeric ({stress_num_vals})\",\n",
    "        'normalized': f\"stress_level_norm ({stress_norm_vals})\",\n",
    "        'numeric_vs_norm_identical': are_identical,\n",
    "        'decision': 'REMOVE stress_level_numeric (redundant with stress_level_norm)' if are_identical else 'REVIEW - May not be identical'\n",
    "    }\n",
    "\n",
    "if redundancies_03:\n",
    "    for key, info in redundancies_03.items():\n",
    "        print(f\"\\n    Potential redundancy: {key}\")\n",
    "        if 'sleep_pair' in key:\n",
    "            print(f\"      Original: {info['original']}\")\n",
    "            print(f\"      Derived: {info['derived']}\")\n",
    "        elif 'stress_level' in key:\n",
    "            print(f\"      Categorical: {info['categorical']}\")\n",
    "            print(f\"      Numeric: {info['numeric']}\")\n",
    "            print(f\"      Normalized: {info['normalized']}\")\n",
    "            if 'numeric_vs_norm_identical' in info:\n",
    "                print(f\"      Are numeric and norm identical? {info['numeric_vs_norm_identical']}\")\n",
    "        print(f\"      Decision: {info['decision']}\")\n",
    "else:\n",
    "    print(\"No redundancies found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed189e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATASET 04 (Selected):\n",
      "\n",
      "   Potential redundancy: sleep_quality_pair\n",
      "      Original: sleep_quality (0-5)\n",
      "      Normalized: sleep_quality_norm (0.000-1.000)\n",
      "      Are they identical? True\n",
      "      Decision: REMOVE original sleep_quality (redundant with normalized version)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n DATASET 04 (Selected):\")\n",
    "redundancies_04 = {}\n",
    "if 'sleep_quality' in df_stress_selected.columns and 'sleep_quality_norm' in df_stress_selected.columns:\n",
    "    sleep_qual_range = f\"{df_stress_selected['sleep_quality'].min()}-{df_stress_selected['sleep_quality'].max()}\"\n",
    "    sleep_qual_norm_range = f\"{df_stress_selected['sleep_quality_norm'].min():.3f}-{df_stress_selected['sleep_quality_norm'].max():.3f}\"\n",
    "    \n",
    "    # Check if normalized version is just original divided by max\n",
    "    sleep_qual_max = df_stress_selected['sleep_quality'].max()\n",
    "    expected_norm = df_stress_selected['sleep_quality'] / sleep_qual_max\n",
    "    are_identical = np.allclose(df_stress_selected['sleep_quality_norm'], expected_norm, rtol=1e-5)\n",
    "    \n",
    "    redundancies_04['sleep_quality_pair'] = {\n",
    "        'original': f\"sleep_quality ({sleep_qual_range})\",\n",
    "        'normalized': f\"sleep_quality_norm ({sleep_qual_norm_range})\",\n",
    "        'are_identical': are_identical,\n",
    "        'decision': 'REMOVE original sleep_quality (redundant with normalized version)' if are_identical else 'REVIEW - May not be simple normalization'\n",
    "    }\n",
    "\n",
    "if 'stress_level' in df_stress_selected.columns and 'stress_level_norm' in df_stress_selected.columns:\n",
    "    stress_orig_vals = sorted(df_stress_selected['stress_level'].unique())\n",
    "    stress_norm_vals = sorted(df_stress_selected['stress_level_norm'].dropna().unique())\n",
    "    \n",
    "    # Check if they are identical\n",
    "    if len(df_stress_selected) == len(df_stress_selected[df_stress_selected['stress_level'].notna()]):\n",
    "        are_identical = (df_stress_selected['stress_level'] == df_stress_selected['stress_level_norm']).all()\n",
    "    else:\n",
    "        are_identical = False\n",
    "    \n",
    "    redundancies_04['stress_level_pair'] = {\n",
    "        'original': f\"stress_level ({stress_orig_vals})\",\n",
    "        'normalized': f\"stress_level_norm ({stress_norm_vals})\",\n",
    "        'are_identical': are_identical,\n",
    "        'decision': 'REMOVE original stress_level (redundant with normalized version)' if are_identical else 'REVIEW - Values may differ'\n",
    "    }\n",
    "\n",
    "if redundancies_04:\n",
    "    for key, info in redundancies_04.items():\n",
    "        print(f\"\\n   Potential redundancy: {key}\")\n",
    "        print(f\"      Original: {info['original']}\")\n",
    "        print(f\"      Normalized: {info['normalized']}\")\n",
    "        if 'are_identical' in info:\n",
    "            print(f\"      Are they identical? {info['are_identical']}\")\n",
    "        print(f\"      Decision: {info['decision']}\")\n",
    "else:\n",
    "    print(\"   No redundancies found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "548bddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "2. REDUNDANT VARIABLES DETAILED ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "DATASET 01:\n",
      "\n",
      "    Potential redundancy: sleep_quality_pair\n",
      "      Original: sleep_quality (4-9)\n",
      "      Normalized: sleep_quality_norm (0.444-1.000)\n",
      "      Decision: KEEP BOTH - Different scales provide different information\n",
      "\n",
      "    Potential redundancy: stress_level_pair\n",
      "      Original: stress_level ([np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)])\n",
      "      Normalized: stress_level_norm ([np.int64(0), np.int64(1), np.int64(2)])\n",
      "      Decision: KEEP BOTH - Original for reference, normalized for model\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. REDUNDANT VARIABLES DETAILED ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. REDUNDANT VARIABLES DETAILED ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDATASET 01:\")\n",
    "redundancies_01 = {}\n",
    "if 'sleep_quality' in df_sleep_norm.columns and 'sleep_quality_norm' in df_sleep_norm.columns:\n",
    "    sleep_qual_range = f\"{df_sleep_norm['sleep_quality'].min()}-{df_sleep_norm['sleep_quality'].max()}\"\n",
    "    sleep_qual_norm_range = f\"{df_sleep_norm['sleep_quality_norm'].min():.3f}-{df_sleep_norm['sleep_quality_norm'].max():.3f}\"\n",
    "    redundancies_01['sleep_quality_pair'] = {\n",
    "        'original': f\"sleep_quality ({sleep_qual_range})\",\n",
    "        'normalized': f\"sleep_quality_norm ({sleep_qual_norm_range})\",\n",
    "        'decision': 'KEEP BOTH - Different scales provide different information'\n",
    "    }\n",
    "\n",
    "if 'stress_level' in df_sleep_norm.columns and 'stress_level_norm' in df_sleep_norm.columns:\n",
    "    stress_orig_vals = sorted(df_sleep_norm['stress_level'].unique())\n",
    "    stress_norm_vals = sorted(df_sleep_norm['stress_level_norm'].dropna().unique())\n",
    "    redundancies_01['stress_level_pair'] = {\n",
    "        'original': f\"stress_level ({stress_orig_vals})\",\n",
    "        'normalized': f\"stress_level_norm ({stress_norm_vals})\",\n",
    "        'decision': 'KEEP BOTH - Original for reference, normalized for model'\n",
    "    }\n",
    "\n",
    "if redundancies_01:\n",
    "    for key, info in redundancies_01.items():\n",
    "        print(f\"\\n    Potential redundancy: {key}\")\n",
    "        print(f\"      Original: {info['original']}\")\n",
    "        print(f\"      Normalized: {info['normalized']}\")\n",
    "        print(f\"      Decision: {info['decision']}\")\n",
    "else:\n",
    "    print(\"    No redundancies found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31bb21fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATASET 03:\n",
      "\n",
      "    Potential redundancy: sleep_pair\n",
      "      Original: sleep_hours (1.4-11.3 hours)\n",
      "      Derived: sleep_quality_norm (0.117-0.942)\n",
      "      Decision: KEEP BOTH - Conceptually different (quantity vs quality)\n",
      "\n",
      "    Potential redundancy: stress_level_triple\n",
      "      Categorical: stress_level (['High', 'Low', 'Moderate'])\n",
      "      Numeric: stress_level_numeric ([np.int64(2), np.int64(5), np.int64(8)])\n",
      "      Normalized: stress_level_norm ([np.int64(0), np.int64(1), np.int64(2)])\n",
      "      Are numeric and norm identical? False\n",
      "      Decision: REVIEW - May not be identical\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n DATASET 03:\")\n",
    "redundancies_03 = {}\n",
    "if 'sleep_hours' in df_lifestyle_norm.columns and 'sleep_quality_norm' in df_lifestyle_norm.columns:\n",
    "    sleep_hours_range = f\"{df_lifestyle_norm['sleep_hours'].min():.1f}-{df_lifestyle_norm['sleep_hours'].max():.1f}\"\n",
    "    sleep_qual_norm_range = f\"{df_lifestyle_norm['sleep_quality_norm'].min():.3f}-{df_lifestyle_norm['sleep_quality_norm'].max():.3f}\"\n",
    "    redundancies_03['sleep_pair'] = {\n",
    "        'original': f\"sleep_hours ({sleep_hours_range} hours)\",\n",
    "        'derived': f\"sleep_quality_norm ({sleep_qual_norm_range})\",\n",
    "        'decision': 'KEEP BOTH - Conceptually different (quantity vs quality)'\n",
    "    }\n",
    "\n",
    "if 'stress_level' in df_lifestyle_norm.columns and 'stress_level_numeric' in df_lifestyle_norm.columns and 'stress_level_norm' in df_lifestyle_norm.columns:\n",
    "    stress_cat_vals = sorted(df_lifestyle_norm['stress_level'].unique())\n",
    "    stress_num_vals = sorted(df_lifestyle_norm['stress_level_numeric'].unique())\n",
    "    stress_norm_vals = sorted(df_lifestyle_norm['stress_level_norm'].dropna().unique())\n",
    "    \n",
    "    # Verify if stress_level_numeric and stress_level_norm are identical\n",
    "    if len(df_lifestyle_norm) == len(df_lifestyle_norm[df_lifestyle_norm['stress_level_numeric'].notna()]):\n",
    "        comparison = (df_lifestyle_norm['stress_level_numeric'] == df_lifestyle_norm['stress_level_norm']).all()\n",
    "        are_identical = comparison if isinstance(comparison, bool) else comparison.sum() == len(df_lifestyle_norm)\n",
    "    else:\n",
    "        are_identical = False\n",
    "    \n",
    "    redundancies_03['stress_level_triple'] = {\n",
    "        'categorical': f\"stress_level ({stress_cat_vals})\",\n",
    "        'numeric': f\"stress_level_numeric ({stress_num_vals})\",\n",
    "        'normalized': f\"stress_level_norm ({stress_norm_vals})\",\n",
    "        'numeric_vs_norm_identical': are_identical,\n",
    "        'decision': 'REMOVE stress_level_numeric (redundant with stress_level_norm)' if are_identical else 'REVIEW - May not be identical'\n",
    "    }\n",
    "\n",
    "if redundancies_03:\n",
    "    for key, info in redundancies_03.items():\n",
    "        print(f\"\\n    Potential redundancy: {key}\")\n",
    "        if 'sleep_pair' in key:\n",
    "            print(f\"      Original: {info['original']}\")\n",
    "            print(f\"      Derived: {info['derived']}\")\n",
    "        elif 'stress_level' in key:\n",
    "            print(f\"      Categorical: {info['categorical']}\")\n",
    "            print(f\"      Numeric: {info['numeric']}\")\n",
    "            print(f\"      Normalized: {info['normalized']}\")\n",
    "            if 'numeric_vs_norm_identical' in info:\n",
    "                print(f\"      Are numeric and norm identical? {info['numeric_vs_norm_identical']}\")\n",
    "        print(f\"      Decision: {info['decision']}\")\n",
    "else:\n",
    "    print(\"    No redundancies found\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5afdbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATASET 04 (Selected):\n",
      "\n",
      "    Potential redundancy: sleep_quality_pair\n",
      "      Original: sleep_quality (0-5)\n",
      "      Normalized: sleep_quality_norm (0.000-1.000)\n",
      "      Are they identical? True\n",
      "      Decision: REMOVE original sleep_quality (redundant with normalized version)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n DATASET 04 (Selected):\")\n",
    "redundancies_04 = {}\n",
    "if 'sleep_quality' in df_stress_selected.columns and 'sleep_quality_norm' in df_stress_selected.columns:\n",
    "    sleep_qual_range = f\"{df_stress_selected['sleep_quality'].min()}-{df_stress_selected['sleep_quality'].max()}\"\n",
    "    sleep_qual_norm_range = f\"{df_stress_selected['sleep_quality_norm'].min():.3f}-{df_stress_selected['sleep_quality_norm'].max():.3f}\"\n",
    "    \n",
    "    # Check if normalized version is just original divided by max\n",
    "    sleep_qual_max = df_stress_selected['sleep_quality'].max()\n",
    "    expected_norm = df_stress_selected['sleep_quality'] / sleep_qual_max\n",
    "    are_identical = np.allclose(df_stress_selected['sleep_quality_norm'], expected_norm, rtol=1e-5)\n",
    "    \n",
    "    redundancies_04['sleep_quality_pair'] = {\n",
    "        'original': f\"sleep_quality ({sleep_qual_range})\",\n",
    "        'normalized': f\"sleep_quality_norm ({sleep_qual_norm_range})\",\n",
    "        'are_identical': are_identical,\n",
    "        'decision': 'REMOVE original sleep_quality (redundant with normalized version)' if are_identical else 'REVIEW - May not be simple normalization'\n",
    "    }\n",
    "\n",
    "if 'stress_level' in df_stress_selected.columns and 'stress_level_norm' in df_stress_selected.columns:\n",
    "    stress_orig_vals = sorted(df_stress_selected['stress_level'].unique())\n",
    "    stress_norm_vals = sorted(df_stress_selected['stress_level_norm'].dropna().unique())\n",
    "    \n",
    "    # Check if they are identical\n",
    "    if len(df_stress_selected) == len(df_stress_selected[df_stress_selected['stress_level'].notna()]):\n",
    "        are_identical = (df_stress_selected['stress_level'] == df_stress_selected['stress_level_norm']).all()\n",
    "    else:\n",
    "        are_identical = False\n",
    "    \n",
    "    redundancies_04['stress_level_pair'] = {\n",
    "        'original': f\"stress_level ({stress_orig_vals})\",\n",
    "        'normalized': f\"stress_level_norm ({stress_norm_vals})\",\n",
    "        'are_identical': are_identical,\n",
    "        'decision': 'REMOVE original stress_level (redundant with normalized version)' if are_identical else 'REVIEW - Values may differ'\n",
    "    }\n",
    "\n",
    "if redundancies_04:\n",
    "    for key, info in redundancies_04.items():\n",
    "        print(f\"\\n    Potential redundancy: {key}\")\n",
    "        print(f\"      Original: {info['original']}\")\n",
    "        print(f\"      Normalized: {info['normalized']}\")\n",
    "        if 'are_identical' in info:\n",
    "            print(f\"      Are they identical? {info['are_identical']}\")\n",
    "        print(f\"      Decision: {info['decision']}\")\n",
    "else:\n",
    "    print(\"    No redundancies found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7fc418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3. DATA TYPE CONSISTENCY CHECK (Common Variables)\n",
      "======================================================================\n",
      "\n",
      "   Common variables (ALL 3 datasets): 4\n",
      "      age                      : {'Dataset_01': 'int64', 'Dataset_03': 'int64', 'Dataset_04': 'int32'} (INCONSISTENT - needs attention)\n",
      "       gender                   : object     (CONSISTENT)\n",
      "       sleep_quality_norm       : float64    (CONSISTENT)\n",
      "       stress_level_norm        : int64      (CONSISTENT)\n",
      "\n",
      "    1 variables have inconsistent data types that need fixing\n",
      "\n",
      "   Common variables (Datasets 01 & 03 only): 1\n",
      "      - stress_level\n",
      "\n",
      "   Common variables (Datasets 01 & 04 only): 1\n",
      "      - sleep_quality\n",
      "\n",
      "   Common variables (Datasets 03 & 04 only): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. DATA TYPE CONSISTENCY CHECK (Common Variables)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "common_vars_all = set(df_sleep_norm.columns) & set(df_lifestyle_norm.columns) & set(df_stress_selected.columns)\n",
    "common_vars_01_03 = set(df_sleep_norm.columns) & set(df_lifestyle_norm.columns)\n",
    "common_vars_01_04 = set(df_sleep_norm.columns) & set(df_stress_selected.columns)\n",
    "common_vars_03_04 = set(df_lifestyle_norm.columns) & set(df_stress_selected.columns)\n",
    "\n",
    "print(f\"\\n   Common variables (ALL 3 datasets): {len(common_vars_all)}\")\n",
    "if common_vars_all:\n",
    "    type_issues = []\n",
    "    for var in sorted(common_vars_all):\n",
    "        dtypes = {}\n",
    "        if var in df_sleep_norm.columns:\n",
    "            dtypes['Dataset_01'] = str(df_sleep_norm[var].dtype)\n",
    "        if var in df_lifestyle_norm.columns:\n",
    "            dtypes['Dataset_03'] = str(df_lifestyle_norm[var].dtype)\n",
    "        if var in df_stress_selected.columns:\n",
    "            dtypes['Dataset_04'] = str(df_stress_selected[var].dtype)\n",
    "        \n",
    "        unique_types = set([v for v in dtypes.values() if v])\n",
    "        is_consistent = len(unique_types) == 1\n",
    "        \n",
    "        if is_consistent:\n",
    "            print(f\"       {var:25s}: {list(unique_types)[0]:10s} (CONSISTENT)\")\n",
    "        else:\n",
    "            type_issues.append((var, dtypes))\n",
    "            print(f\"      {var:25s}: {dtypes} (INCONSISTENT - needs attention)\")\n",
    "    if not type_issues:\n",
    "        print(\"\\n   All common variables have consistent data types!\")\n",
    "    else:\n",
    "        print(f\"\\n    {len(type_issues)} variables have inconsistent data types that need fixing\")\n",
    "else:\n",
    "    print(\"    No common variables found in all 3 datasets\")\n",
    "\n",
    "print(f\"\\n   Common variables (Datasets 01 & 03 only): {len(common_vars_01_03 - common_vars_all)}\")\n",
    "if common_vars_01_03 - common_vars_all:\n",
    "    for var in sorted(common_vars_01_03 - common_vars_all):\n",
    "        print(f\"      - {var}\")\n",
    "\n",
    "print(f\"\\n   Common variables (Datasets 01 & 04 only): {len(common_vars_01_04 - common_vars_all)}\")\n",
    "if common_vars_01_04 - common_vars_all:\n",
    "    for var in sorted(common_vars_01_04 - common_vars_all):\n",
    "        print(f\"      - {var}\")\n",
    "\n",
    "print(f\"\\n   Common variables (Datasets 03 & 04 only): {len(common_vars_03_04 - common_vars_all)}\")\n",
    "if common_vars_03_04 - common_vars_all:\n",
    "    for var in sorted(common_vars_03_04 - common_vars_all):\n",
    "        print(f\"      - {var}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f7b9667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "4. VALUE RANGES AND DISTRIBUTIONS (Key Variables)\n",
      "======================================================================\n",
      "\n",
      "    age:\n",
      "      Dataset 01: min=27, max=59, mean=42.184, std=8.673\n",
      "      Dataset 03: min=18, max=64, mean=41.230, std=13.428\n",
      "      Dataset 04: min=18, max=29, mean=23.434, std=3.519\n",
      "\n",
      "    gender:\n",
      "      Dataset 01: {'Male': np.int64(189), 'Female': np.int64(185)}\n",
      "      Dataset 03: {'Female': np.int64(1024), 'Other': np.int64(996), 'Male': np.int64(980)}\n",
      "      Dataset 04: {'Male': np.int64(550), 'Female': np.int64(550)}\n",
      "\n",
      "    stress_level_norm:\n",
      "      Dataset 01: min=0, max=2, mean=0.997, std=0.615\n",
      "      Dataset 03: min=0, max=2, mean=0.998, std=0.819\n",
      "      Dataset 04: min=0, max=2, mean=0.996, std=0.822\n",
      "\n",
      "    sleep_quality_norm:\n",
      "      Dataset 01: min=0.4444444444444444, max=1.0, mean=0.813, std=0.133\n",
      "      Dataset 03: min=0.11666666666666665, max=0.9416666666666668, mean=0.540, std=0.125\n",
      "      Dataset 04: min=0.0, max=1.0, mean=0.532, std=0.310\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. VALUE RANGES AND DISTRIBUTIONS (Key Variables)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "key_vars = ['age', 'gender', 'stress_level_norm', 'sleep_quality_norm']\n",
    "\n",
    "for var in key_vars:\n",
    "    print(f\"\\n    {var}:\")\n",
    "    \n",
    "    if var in df_sleep_norm.columns:\n",
    "        vals = df_sleep_norm[var].dropna()\n",
    "        if var == 'gender':\n",
    "            print(f\"      Dataset 01: {dict(vals.value_counts())}\")\n",
    "        else:\n",
    "            print(f\"      Dataset 01: min={vals.min()}, max={vals.max()}, mean={vals.mean():.3f}, std={vals.std():.3f}\")\n",
    "    \n",
    "    if var in df_lifestyle_norm.columns:\n",
    "        vals = df_lifestyle_norm[var].dropna()\n",
    "        if var == 'gender':\n",
    "            print(f\"      Dataset 03: {dict(vals.value_counts())}\")\n",
    "        else:\n",
    "            print(f\"      Dataset 03: min={vals.min()}, max={vals.max()}, mean={vals.mean():.3f}, std={vals.std():.3f}\")\n",
    "    \n",
    "    if var in df_stress_selected.columns:\n",
    "        vals = df_stress_selected[var].dropna()\n",
    "        if var == 'gender':\n",
    "            print(f\"      Dataset 04: {dict(vals.value_counts())}\")\n",
    "        else:\n",
    "            print(f\"      Dataset 04: min={vals.min()}, max={vals.max()}, mean={vals.mean():.3f}, std={vals.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65123ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5. MERGE COMPATIBILITY ASSESSMENT\n",
      "======================================================================\n",
      "\n",
      "   Merge Strategy Analysis:\n",
      "      Common variables (ALL): 4\n",
      "      Sufficient common variables for horizontal merge\n",
      "      Recommended merge keys: ['age', 'gender', 'sleep_quality_norm', 'stress_level_norm']\n",
      "\n",
      "   Potential Issues:\n",
      "       Data type inconsistency in 'age': 01=int64, 03=int64, 04=int32\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5. MERGE COMPATIBILITY ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n   Merge Strategy Analysis:\")\n",
    "print(f\"      Common variables (ALL): {len(common_vars_all)}\")\n",
    "if len(common_vars_all) >= 3:\n",
    "    print(f\"      Sufficient common variables for horizontal merge\")\n",
    "    print(f\"      Recommended merge keys: {sorted(common_vars_all)}\")\n",
    "else:\n",
    "    print(f\"       Limited common variables - may need vertical concatenation\")\n",
    "    print(f\"      Alternative: Create synthetic keys or use vertical concatenation\")\n",
    "\n",
    "print(\"\\n   Potential Issues:\")\n",
    "issues = []\n",
    "\n",
    "# Check for missing values in key variables\n",
    "for var in ['age', 'gender', 'stress_level_norm']:\n",
    "    missing_01 = df_sleep_norm[var].isnull().sum() if var in df_sleep_norm.columns else 'N/A'\n",
    "    missing_03 = df_lifestyle_norm[var].isnull().sum() if var in df_lifestyle_norm.columns else 'N/A'\n",
    "    missing_04 = df_stress_selected[var].isnull().sum() if var in df_stress_selected.columns else 'N/A'\n",
    "    \n",
    "    if missing_01 != 0 or missing_03 != 0 or missing_04 != 0:\n",
    "        issues.append(f\"Missing values in '{var}': Dataset 01={missing_01}, 03={missing_03}, 04={missing_04}\")\n",
    "\n",
    "# Check for data type inconsistencies in key variables\n",
    "for var in common_vars_all:\n",
    "    if var in df_sleep_norm.columns and var in df_lifestyle_norm.columns and var in df_stress_selected.columns:\n",
    "        type_01 = str(df_sleep_norm[var].dtype)\n",
    "        type_03 = str(df_lifestyle_norm[var].dtype)\n",
    "        type_04 = str(df_stress_selected[var].dtype)\n",
    "        \n",
    "        if type_01 != type_03 or type_01 != type_04 or type_03 != type_04:\n",
    "            issues.append(f\"Data type inconsistency in '{var}': 01={type_01}, 03={type_03}, 04={type_04}\")\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f\"       {issue}\")\n",
    "else:\n",
    "    print(\"      No major issues identified for merge compatibility\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4ac74f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "   Dataset 01: 374 rows √ó 14 columns\n",
      "   Dataset 03: 3,000 rows √ó 14 columns\n",
      "   Dataset 04: 1,100 rows √ó 21 columns\n",
      "   Total rows: 4,474\n",
      "   Common variables (ALL): 4\n",
      "   Total unique variables across all datasets: 39\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "total_cols_01 = len(df_sleep_norm.columns)\n",
    "total_cols_03 = len(df_lifestyle_norm.columns)\n",
    "total_cols_04 = len(df_stress_selected.columns)\n",
    "total_rows = df_sleep_norm.shape[0] + df_lifestyle_norm.shape[0] + df_stress_selected.shape[0]\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"   Dataset 01: {df_sleep_norm.shape[0]:,} rows √ó {total_cols_01} columns\")\n",
    "print(f\"   Dataset 03: {df_lifestyle_norm.shape[0]:,} rows √ó {total_cols_03} columns\")\n",
    "print(f\"   Dataset 04: {df_stress_selected.shape[0]:,} rows √ó {total_cols_04} columns\")\n",
    "print(f\"   Total rows: {total_rows:,}\")\n",
    "print(f\"   Common variables (ALL): {len(common_vars_all)}\")\n",
    "print(f\"   Total unique variables across all datasets: {len(set(df_sleep_norm.columns) | set(df_lifestyle_norm.columns) | set(df_stress_selected.columns))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f454308a",
   "metadata": {},
   "source": [
    "## 7. Apply Pre-Merge Corrections\n",
    "\n",
    "Before performing the vertical merge, we need to:\n",
    "1. Remove redundant variables identified in the analysis\n",
    "2. Fix data type inconsistencies\n",
    "3. Verify critical assumptions\n",
    "4. Prepare datasets for concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d60e25d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFICATION: stress_level_numeric vs stress_level_norm (Dataset 03)\n",
      "======================================================================\n",
      "\n",
      "1. Data Summary:\n",
      "   Total rows: 3000\n",
      "   stress_level_numeric unique values: [np.int64(2), np.int64(5), np.int64(8)]\n",
      "   stress_level_norm unique values: [np.int64(0), np.int64(1), np.int64(2)]\n",
      "\n",
      "2. Value Distribution Comparison:\n",
      "   stress_level_numeric distribution:\n",
      "stress_level_numeric\n",
      "2    1008\n",
      "5     990\n",
      "8    1002\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   stress_level_norm distribution:\n",
      "stress_level_norm\n",
      "0    1008\n",
      "1     990\n",
      "2    1002\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. Correlation Analysis:\n",
      "   Pearson correlation: 1.0000\n",
      "\n",
      "4. Mapping Analysis:\n",
      "stress_level_numeric     2    5     8   All\n",
      "stress_level_norm        0    1     2      \n",
      "stress_level                               \n",
      "High                     0    0  1002  1002\n",
      "Low                   1008    0     0  1008\n",
      "Moderate                 0  990     0   990\n",
      "All                   1008  990  1002  3000\n",
      "\n",
      "5. Decision:\n",
      "   - stress_level_numeric has values: 2, 5, 8 (original numeric scale)\n",
      "   - stress_level_norm has values: 0, 1, 2 (normalized scale)\n",
      "   - They represent the same information but on different scales\n",
      "   - RECOMMENDATION: Remove stress_level_numeric (keep stress_level_norm for consistency)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# VERIFICATION: stress_level_numeric vs stress_level_norm\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICATION: stress_level_numeric vs stress_level_norm (Dataset 03)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'stress_level_numeric' in df_lifestyle_norm.columns and 'stress_level_norm' in df_lifestyle_norm.columns:\n",
    "    print(f\"\\n1. Data Summary:\")\n",
    "    print(f\"   Total rows: {len(df_lifestyle_norm)}\")\n",
    "    print(f\"   stress_level_numeric unique values: {sorted(df_lifestyle_norm['stress_level_numeric'].unique())}\")\n",
    "    print(f\"   stress_level_norm unique values: {sorted(df_lifestyle_norm['stress_level_norm'].dropna().unique())}\")\n",
    "    \n",
    "    print(f\"\\n2. Value Distribution Comparison:\")\n",
    "    print(f\"   stress_level_numeric distribution:\")\n",
    "    print(df_lifestyle_norm['stress_level_numeric'].value_counts().sort_index())\n",
    "    print(f\"\\n   stress_level_norm distribution:\")\n",
    "    print(df_lifestyle_norm['stress_level_norm'].value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\n3. Correlation Analysis:\")\n",
    "    corr_val = df_lifestyle_norm['stress_level_numeric'].corr(df_lifestyle_norm['stress_level_norm'])\n",
    "    print(f\"   Pearson correlation: {corr_val:.4f}\")\n",
    "    \n",
    "    print(f\"\\n4. Mapping Analysis:\")\n",
    "    # Create a cross-tabulation to see the mapping\n",
    "    mapping_df = pd.crosstab(\n",
    "        df_lifestyle_norm['stress_level'], \n",
    "        [df_lifestyle_norm['stress_level_numeric'], df_lifestyle_norm['stress_level_norm']],\n",
    "        margins=True\n",
    "    )\n",
    "    print(mapping_df)\n",
    "    \n",
    "    print(f\"\\n5. Decision:\")\n",
    "    print(f\"   - stress_level_numeric has values: 2, 5, 8 (original numeric scale)\")\n",
    "    print(f\"   - stress_level_norm has values: 0, 1, 2 (normalized scale)\")\n",
    "    print(f\"   - They represent the same information but on different scales\")\n",
    "    print(f\"   - RECOMMENDATION: Remove stress_level_numeric (keep stress_level_norm for consistency)\")\n",
    "else:\n",
    "    print(\"   Variables not found or already removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07a07ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create copies for corrections\n",
    "df_sleep_final = df_sleep_norm.copy()\n",
    "df_lifestyle_final = df_lifestyle_norm.copy()\n",
    "df_stress_final = df_stress_selected.copy()\n",
    "\n",
    "corrections_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7adbd242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Dataset 03 Corrections:\n",
      "   ‚úì Removed 'stress_level_numeric'\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CORRECTION 1: Dataset 03 - Remove stress_level_numeric\n",
    "# ============================================\n",
    "print(\"\\n1. Dataset 03 Corrections:\")\n",
    "if 'stress_level_numeric' in df_lifestyle_final.columns:\n",
    "    df_lifestyle_final = df_lifestyle_final.drop(columns=['stress_level_numeric'])\n",
    "    corrections_log.append(\"Dataset 03: Removed 'stress_level_numeric' (redundant with stress_level_norm)\")\n",
    "    print(\"   ‚úì Removed 'stress_level_numeric'\")\n",
    "else:\n",
    "    print(\"   - 'stress_level_numeric' not found (may already be removed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "095e1e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Removed 'sleep_hours_original'\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CORRECTION 2: Dataset 03 - Remove sleep_hours_original\n",
    "# ============================================\n",
    "if 'sleep_hours_original' in df_lifestyle_final.columns:\n",
    "    df_lifestyle_final = df_lifestyle_final.drop(columns=['sleep_hours_original'])\n",
    "    corrections_log.append(\"Dataset 03: Removed 'sleep_hours_original' (duplicate of sleep_hours)\")\n",
    "    print(\"   ‚úì Removed 'sleep_hours_original'\")\n",
    "else:\n",
    "    print(\"   - 'sleep_hours_original' not found (may already be removed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "757be338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Dataset 04 Corrections:\n",
      "   ‚úì Removed 'sleep_quality' (original)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# CORRECTION 3: Dataset 04 - Remove sleep_quality (original)\n",
    "# ============================================\n",
    "print(\"\\n2. Dataset 04 Corrections:\")\n",
    "if 'sleep_quality' in df_stress_final.columns and 'sleep_quality_norm' in df_stress_final.columns:\n",
    "    df_stress_final = df_stress_final.drop(columns=['sleep_quality'])\n",
    "    corrections_log.append(\"Dataset 04: Removed 'sleep_quality' (redundant with sleep_quality_norm)\")\n",
    "    print(\"   ‚úì Removed 'sleep_quality' (original)\")\n",
    "else:\n",
    "    if 'sleep_quality' not in df_stress_final.columns:\n",
    "        print(\"   - 'sleep_quality' already removed or not in selected variables\")\n",
    "    else:\n",
    "        print(\"   - 'sleep_quality_norm' not found, keeping original\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64578c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Converted 'age' from int32 to int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CORRECTION 4: Dataset 04 - Convert age to int64\n",
    "# ============================================\n",
    "if 'age' in df_stress_final.columns:\n",
    "    original_dtype = df_stress_final['age'].dtype\n",
    "    if original_dtype != 'int64':\n",
    "        df_stress_final['age'] = df_stress_final['age'].astype('int64')\n",
    "        corrections_log.append(f\"Dataset 04: Converted 'age' from {original_dtype} to int64\")\n",
    "        print(f\"   ‚úì Converted 'age' from {original_dtype} to int64\")\n",
    "    else:\n",
    "        print(f\"   - 'age' already int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b5c462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VERIFICATION OF CORRECTIONS\n",
      "======================================================================\n",
      "\n",
      "Dataset 01 Final Shape: (374, 14)\n",
      "Dataset 03 Final Shape: (3000, 12)\n",
      "Dataset 04 Final Shape: (1100, 20)\n",
      "\n",
      "Dataset 03 Columns (12):\n",
      "    1. age\n",
      "    2. gender\n",
      "    3. exercise_level\n",
      "    4. diet_type\n",
      "    5. sleep_hours\n",
      "    6. stress_level\n",
      "    7. work_hours\n",
      "    8. screen_time\n",
      "    9. social_interaction\n",
      "   10. happiness_score\n",
      "   11. sleep_quality_norm\n",
      "   12. stress_level_norm\n",
      "\n",
      "Dataset 04 Columns (20):\n",
      "    1. anxiety_level\n",
      "    2. self_esteem\n",
      "    3. depression\n",
      "    4. mental_health_history\n",
      "    5. headache\n",
      "    6. breathing_problem\n",
      "    7. sleep_quality_norm\n",
      "    8. noise_level\n",
      "    9. living_conditions\n",
      "   10. safety\n",
      "   11. basic_needs\n",
      "   12. mental_health_index\n",
      "   13. protective_factors_score\n",
      "   14. stress_risk_score\n",
      "   15. environmental_quality_score\n",
      "   16. physical_symptoms_score\n",
      "   17. stress_level_norm\n",
      "   18. age\n",
      "   19. gender\n",
      "   20. social_support\n",
      "\n",
      "Age Data Types (should all be int64):\n",
      "   Dataset 01: int64\n",
      "   Dataset 03: int64\n",
      "   Dataset 04: int64\n"
     ]
    }
   ],
   "source": [
    "# VERIFY CORRECTIONS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFICATION OF CORRECTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset 01 Final Shape: {df_sleep_final.shape}\")\n",
    "print(f\"Dataset 03 Final Shape: {df_lifestyle_final.shape}\")\n",
    "print(f\"Dataset 04 Final Shape: {df_stress_final.shape}\")\n",
    "\n",
    "print(f\"\\nDataset 03 Columns ({len(df_lifestyle_final.columns)}):\")\n",
    "for i, col in enumerate(df_lifestyle_final.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nDataset 04 Columns ({len(df_stress_final.columns)}):\")\n",
    "for i, col in enumerate(df_stress_final.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# Verify age type consistency\n",
    "print(f\"\\nAge Data Types (should all be int64):\")\n",
    "if 'age' in df_sleep_final.columns:\n",
    "    print(f\"   Dataset 01: {df_sleep_final['age'].dtype}\")\n",
    "if 'age' in df_lifestyle_final.columns:\n",
    "    print(f\"   Dataset 03: {df_lifestyle_final['age'].dtype}\")\n",
    "if 'age' in df_stress_final.columns:\n",
    "    print(f\"   Dataset 04: {df_stress_final['age'].dtype}\")\n",
    "\n",
    "# Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9956a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Corrections Applied: 4\n",
      "   ‚Ä¢ Dataset 03: Removed 'stress_level_numeric' (redundant with stress_level_norm)\n",
      "   ‚Ä¢ Dataset 03: Removed 'sleep_hours_original' (duplicate of sleep_hours)\n",
      "   ‚Ä¢ Dataset 04: Removed 'sleep_quality' (redundant with sleep_quality_norm)\n",
      "   ‚Ä¢ Dataset 04: Converted 'age' from int32 to int64\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Corrections Applied: {len(corrections_log)}\")\n",
    "for correction in corrections_log:\n",
    "    print(f\"   ‚Ä¢ {correction}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c586e5",
   "metadata": {},
   "source": [
    "## 8. Vertical Merge (Concatenation)\n",
    "\n",
    "### Strategy\n",
    "- Use `pd.concat()` with `axis=0` (vertical concatenation)\n",
    "- Add `dataset_source` column to identify origin\n",
    "- Handle non-matching columns by filling with NaN\n",
    "- Preserve all rows from all datasets\n",
    "\n",
    "### Expected Result\n",
    "- Total rows: 374 + 3,000 + 1,100 = 4,474\n",
    "- Total columns: ~40 (union of all columns)\n",
    "- Missing values: Expected in columns that don't exist in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f546033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dataset source identifier before merge\n",
    "df_sleep_final['dataset_source'] = '01_sleep_health'\n",
    "df_lifestyle_final['dataset_source'] = '03_mental_health'\n",
    "df_stress_final['dataset_source'] = '04_stress_level'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83618f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Adding dataset_source identifier:\n",
      "   Dataset 01: 01_sleep_health\n",
      "   Dataset 03: 03_mental_health\n",
      "   Dataset 04: 04_stress_level\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Adding dataset_source identifier:\")\n",
    "print(f\"   Dataset 01: {df_sleep_final['dataset_source'].unique()[0]}\")\n",
    "print(f\"   Dataset 03: {df_lifestyle_final['dataset_source'].unique()[0]}\")\n",
    "print(f\"   Dataset 04: {df_stress_final['dataset_source'].unique()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aabd9886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Column Analysis:\n",
      "   Total unique columns across all datasets: 38\n",
      "   Columns in Dataset 01: 15\n",
      "   Columns in Dataset 03: 13\n",
      "   Columns in Dataset 04: 21\n"
     ]
    }
   ],
   "source": [
    "# Get all unique column names\n",
    "all_columns = set(df_sleep_final.columns) | set(df_lifestyle_final.columns) | set(df_stress_final.columns)\n",
    "print(f\"\\n2. Column Analysis:\")\n",
    "print(f\"   Total unique columns across all datasets: {len(all_columns)}\")\n",
    "print(f\"   Columns in Dataset 01: {len(df_sleep_final.columns)}\")\n",
    "print(f\"   Columns in Dataset 03: {len(df_lifestyle_final.columns)}\")\n",
    "print(f\"   Columns in Dataset 04: {len(df_stress_final.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed596cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Performing concatenation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>sleep_duration</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>physical_activity</th>\n",
       "      <th>stress_level</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>blood_pressure_diastolic</th>\n",
       "      <th>sleep_quality_category</th>\n",
       "      <th>stress_category</th>\n",
       "      <th>sleep_quality_norm</th>\n",
       "      <th>stress_level_norm</th>\n",
       "      <th>dataset_source</th>\n",
       "      <th>exercise_level</th>\n",
       "      <th>diet_type</th>\n",
       "      <th>sleep_hours</th>\n",
       "      <th>work_hours</th>\n",
       "      <th>screen_time</th>\n",
       "      <th>social_interaction</th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>anxiety_level</th>\n",
       "      <th>self_esteem</th>\n",
       "      <th>depression</th>\n",
       "      <th>mental_health_history</th>\n",
       "      <th>headache</th>\n",
       "      <th>breathing_problem</th>\n",
       "      <th>noise_level</th>\n",
       "      <th>living_conditions</th>\n",
       "      <th>safety</th>\n",
       "      <th>basic_needs</th>\n",
       "      <th>mental_health_index</th>\n",
       "      <th>protective_factors_score</th>\n",
       "      <th>stress_risk_score</th>\n",
       "      <th>environmental_quality_score</th>\n",
       "      <th>physical_symptoms_score</th>\n",
       "      <th>social_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>77.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>01_sleep_health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Normal</td>\n",
       "      <td>75.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>High</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>01_sleep_health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Normal</td>\n",
       "      <td>75.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>High</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>01_sleep_health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Obese</td>\n",
       "      <td>85.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Poor</td>\n",
       "      <td>High</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>2</td>\n",
       "      <td>01_sleep_health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Obese</td>\n",
       "      <td>85.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Poor</td>\n",
       "      <td>High</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>2</td>\n",
       "      <td>01_sleep_health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4469</th>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>04_stress_level</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.521164</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4470</th>\n",
       "      <td>Male</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>04_stress_level</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.362434</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>Male</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>04_stress_level</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4472</th>\n",
       "      <td>Female</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2</td>\n",
       "      <td>04_stress_level</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>04_stress_level</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.706349</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4474 rows √ó 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  age            Occupation  sleep_duration  sleep_quality  \\\n",
       "0       Male   27     Software Engineer             6.1            6.0   \n",
       "1       Male   28                Doctor             6.2            6.0   \n",
       "2       Male   28                Doctor             6.2            6.0   \n",
       "3       Male   28  Sales Representative             5.9            4.0   \n",
       "4       Male   28  Sales Representative             5.9            4.0   \n",
       "...      ...  ...                   ...             ...            ...   \n",
       "4469    Male   20                   NaN             NaN            NaN   \n",
       "4470    Male   22                   NaN             NaN            NaN   \n",
       "4471    Male   22                   NaN             NaN            NaN   \n",
       "4472  Female   22                   NaN             NaN            NaN   \n",
       "4473    Male   20                   NaN             NaN            NaN   \n",
       "\n",
       "      physical_activity stress_level bmi_category  heart_rate  \\\n",
       "0                  42.0            6   Overweight        77.0   \n",
       "1                  60.0            8       Normal        75.0   \n",
       "2                  60.0            8       Normal        75.0   \n",
       "3                  30.0            8        Obese        85.0   \n",
       "4                  30.0            8        Obese        85.0   \n",
       "...                 ...          ...          ...         ...   \n",
       "4469                NaN          NaN          NaN         NaN   \n",
       "4470                NaN          NaN          NaN         NaN   \n",
       "4471                NaN          NaN          NaN         NaN   \n",
       "4472                NaN          NaN          NaN         NaN   \n",
       "4473                NaN          NaN          NaN         NaN   \n",
       "\n",
       "      blood_pressure_diastolic sleep_quality_category stress_category  \\\n",
       "0                         83.0               Moderate          Medium   \n",
       "1                         80.0               Moderate            High   \n",
       "2                         80.0               Moderate            High   \n",
       "3                         90.0                   Poor            High   \n",
       "4                         90.0                   Poor            High   \n",
       "...                        ...                    ...             ...   \n",
       "4469                       NaN                    NaN             NaN   \n",
       "4470                       NaN                    NaN             NaN   \n",
       "4471                       NaN                    NaN             NaN   \n",
       "4472                       NaN                    NaN             NaN   \n",
       "4473                       NaN                    NaN             NaN   \n",
       "\n",
       "      sleep_quality_norm  stress_level_norm   dataset_source exercise_level  \\\n",
       "0               0.666667                  1  01_sleep_health            NaN   \n",
       "1               0.666667                  2  01_sleep_health            NaN   \n",
       "2               0.666667                  2  01_sleep_health            NaN   \n",
       "3               0.444444                  2  01_sleep_health            NaN   \n",
       "4               0.444444                  2  01_sleep_health            NaN   \n",
       "...                  ...                ...              ...            ...   \n",
       "4469            0.600000                  1  04_stress_level            NaN   \n",
       "4470            0.000000                  2  04_stress_level            NaN   \n",
       "4471            1.000000                  0  04_stress_level            NaN   \n",
       "4472            0.200000                  2  04_stress_level            NaN   \n",
       "4473            0.000000                  2  04_stress_level            NaN   \n",
       "\n",
       "     diet_type  sleep_hours  work_hours  screen_time  social_interaction  \\\n",
       "0          NaN          NaN         NaN          NaN                 NaN   \n",
       "1          NaN          NaN         NaN          NaN                 NaN   \n",
       "2          NaN          NaN         NaN          NaN                 NaN   \n",
       "3          NaN          NaN         NaN          NaN                 NaN   \n",
       "4          NaN          NaN         NaN          NaN                 NaN   \n",
       "...        ...          ...         ...          ...                 ...   \n",
       "4469       NaN          NaN         NaN          NaN                 NaN   \n",
       "4470       NaN          NaN         NaN          NaN                 NaN   \n",
       "4471       NaN          NaN         NaN          NaN                 NaN   \n",
       "4472       NaN          NaN         NaN          NaN                 NaN   \n",
       "4473       NaN          NaN         NaN          NaN                 NaN   \n",
       "\n",
       "      happiness_score  anxiety_level  self_esteem  depression  \\\n",
       "0                 NaN            NaN          NaN         NaN   \n",
       "1                 NaN            NaN          NaN         NaN   \n",
       "2                 NaN            NaN          NaN         NaN   \n",
       "3                 NaN            NaN          NaN         NaN   \n",
       "4                 NaN            NaN          NaN         NaN   \n",
       "...               ...            ...          ...         ...   \n",
       "4469              NaN           11.0         17.0        14.0   \n",
       "4470              NaN            9.0         12.0         8.0   \n",
       "4471              NaN            4.0         26.0         3.0   \n",
       "4472              NaN           21.0          0.0        19.0   \n",
       "4473              NaN           18.0          6.0        15.0   \n",
       "\n",
       "      mental_health_history  headache  breathing_problem  noise_level  \\\n",
       "0                       NaN       NaN                NaN          NaN   \n",
       "1                       NaN       NaN                NaN          NaN   \n",
       "2                       NaN       NaN                NaN          NaN   \n",
       "3                       NaN       NaN                NaN          NaN   \n",
       "4                       NaN       NaN                NaN          NaN   \n",
       "...                     ...       ...                ...          ...   \n",
       "4469                    0.0       3.0                2.0          2.0   \n",
       "4470                    0.0       0.0                0.0          0.0   \n",
       "4471                    0.0       1.0                2.0          2.0   \n",
       "4472                    1.0       5.0                4.0          3.0   \n",
       "4473                    1.0       3.0                3.0          3.0   \n",
       "\n",
       "      living_conditions  safety  basic_needs  mental_health_index  \\\n",
       "0                   NaN     NaN          NaN                  NaN   \n",
       "1                   NaN     NaN          NaN                  NaN   \n",
       "2                   NaN     NaN          NaN                  NaN   \n",
       "3                   NaN     NaN          NaN                  NaN   \n",
       "4                   NaN     NaN          NaN                  NaN   \n",
       "...                 ...     ...          ...                  ...   \n",
       "4469                2.0     2.0          3.0             0.521164   \n",
       "4470                1.0     3.0          4.0             0.362434   \n",
       "4471                3.0     4.0          4.0             0.150794   \n",
       "4472                1.0     1.0          1.0             0.851852   \n",
       "4473                0.0     4.0          3.0             0.706349   \n",
       "\n",
       "      protective_factors_score  stress_risk_score  \\\n",
       "0                          NaN                NaN   \n",
       "1                          NaN                NaN   \n",
       "2                          NaN                NaN   \n",
       "3                          NaN                NaN   \n",
       "4                          NaN                NaN   \n",
       "...                        ...                ...   \n",
       "4469                  0.641667               0.50   \n",
       "4470                  0.183333               0.40   \n",
       "4471                  0.966667               0.20   \n",
       "4472                  0.233333               0.85   \n",
       "4473                  0.283333               0.80   \n",
       "\n",
       "      environmental_quality_score  physical_symptoms_score  social_support  \n",
       "0                             NaN                      NaN             NaN  \n",
       "1                             NaN                      NaN             NaN  \n",
       "2                             NaN                      NaN             NaN  \n",
       "3                             NaN                      NaN             NaN  \n",
       "4                             NaN                      NaN             NaN  \n",
       "...                           ...                      ...             ...  \n",
       "4469                     0.466667                      0.5             3.0  \n",
       "4470                     0.533333                      0.0             1.0  \n",
       "4471                     0.733333                      0.3             3.0  \n",
       "4472                     0.200000                      0.9             1.0  \n",
       "4473                     0.466667                      0.6             1.0  \n",
       "\n",
       "[4474 rows x 38 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform vertical concatenation\n",
    "print(f\"\\n3. Performing concatenation...\")\n",
    "df_unified = pd.concat(\n",
    "    [df_sleep_final, df_lifestyle_final, df_stress_final],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    "    sort=False\n",
    ")\n",
    "\n",
    "df_unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ce026cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Expected total rows: 4,474\n",
      "   Actual total rows: 4,474\n",
      "   Total columns: 38\n",
      "   Missing values: 102,302\n"
     ]
    }
   ],
   "source": [
    "#Display merge results\n",
    "print(f\"   Expected total rows: {len(df_sleep_final) + len(df_lifestyle_final) + len(df_stress_final):,}\")\n",
    "print(f\"   Actual total rows: {len(df_unified):,}\")\n",
    "print(f\"   Total columns: {len(df_unified.columns)}\")\n",
    "print(f\"   Missing values: {df_unified.isnull().sum().sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be4d5996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Dataset Source Distribution:\n",
      "dataset_source\n",
      "01_sleep_health      374\n",
      "03_mental_health    3000\n",
      "04_stress_level     1100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset_source distribution\n",
    "print(f\"\\n5. Dataset Source Distribution:\")\n",
    "print(df_unified['dataset_source'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1e5c4",
   "metadata": {},
   "source": [
    "## 8.1 Pre-Validation Review\n",
    "\n",
    "Before performing detailed post-merge validation, let's conduct a comprehensive review of:\n",
    "- Dataset structure and completeness\n",
    "- Column-by-column analysis\n",
    "- Missing values pattern analysis\n",
    "- Common variables verification\n",
    "- Data integrity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ece1307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRE-VALIDATION REVIEW\n",
      "======================================================================\n",
      "\n",
      "1. BASIC STRUCTURE:\n",
      "   Total Rows: 4,474\n",
      "   Total Columns: 38\n",
      "   Total Cells: 170,012\n",
      "   Missing Values: 102,302\n",
      "   Missing Percentage: 60.17%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRE-VALIDATION REVIEW\")\n",
    "print(\"=\"*70)\n",
    "# 1. Basic Structure Review\n",
    "print(\"\\n1. BASIC STRUCTURE:\")\n",
    "print(f\"   Total Rows: {len(df_unified):,}\")\n",
    "print(f\"   Total Columns: {len(df_unified.columns)}\")\n",
    "print(f\"   Total Cells: {len(df_unified) * len(df_unified.columns):,}\")\n",
    "print(f\"   Missing Values: {df_unified.isnull().sum().sum():,}\")\n",
    "print(f\"   Missing Percentage: {(df_unified.isnull().sum().sum() / (len(df_unified) * len(df_unified.columns))) * 100:.2f}%\")\n",
    "\n",
    "# 2. C\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0f82ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. COLUMN OVERVIEW:\n",
      "   All columns (38):\n",
      "    1. gender                              [object    ] 4,474 values (Complete)\n",
      "    2. age                                 [int64     ] 4,474 values (Complete)\n",
      "    3. Occupation                          [object    ]   374 values ( 91.6% missing)\n",
      "    4. sleep_duration                      [float64   ]   374 values ( 91.6% missing)\n",
      "    5. sleep_quality                       [float64   ]   374 values ( 91.6% missing)\n",
      "    6. physical_activity                   [float64   ]   374 values ( 91.6% missing)\n",
      "    7. stress_level                        [object    ] 3,374 values ( 24.6% missing)\n",
      "    8. bmi_category                        [object    ]   374 values ( 91.6% missing)\n",
      "    9. heart_rate                          [float64   ]   374 values ( 91.6% missing)\n",
      "   10. blood_pressure_diastolic            [float64   ]   374 values ( 91.6% missing)\n",
      "   11. sleep_quality_category              [object    ]   374 values ( 91.6% missing)\n",
      "   12. stress_category                     [object    ]   374 values ( 91.6% missing)\n",
      "   13. sleep_quality_norm                  [float64   ] 4,474 values (Complete)\n",
      "   14. stress_level_norm                   [int64     ] 4,474 values (Complete)\n",
      "   15. dataset_source                      [object    ] 4,474 values (Complete)\n",
      "   16. exercise_level                      [object    ] 3,000 values ( 32.9% missing)\n",
      "   17. diet_type                           [object    ] 3,000 values ( 32.9% missing)\n",
      "   18. sleep_hours                         [float64   ] 3,000 values ( 32.9% missing)\n",
      "   19. work_hours                          [float64   ] 3,000 values ( 32.9% missing)\n",
      "   20. screen_time                         [float64   ] 3,000 values ( 32.9% missing)\n",
      "   21. social_interaction                  [float64   ] 3,000 values ( 32.9% missing)\n",
      "   22. happiness_score                     [float64   ] 3,000 values ( 32.9% missing)\n",
      "   23. anxiety_level                       [float64   ] 1,100 values ( 75.4% missing)\n",
      "   24. self_esteem                         [float64   ] 1,100 values ( 75.4% missing)\n",
      "   25. depression                          [float64   ] 1,100 values ( 75.4% missing)\n",
      "   26. mental_health_history               [float64   ] 1,100 values ( 75.4% missing)\n",
      "   27. headache                            [float64   ] 1,100 values ( 75.4% missing)\n",
      "   28. breathing_problem                   [float64   ] 1,100 values ( 75.4% missing)\n",
      "   29. noise_level                         [float64   ] 1,100 values ( 75.4% missing)\n",
      "   30. living_conditions                   [float64   ] 1,100 values ( 75.4% missing)\n",
      "   31. safety                              [float64   ] 1,100 values ( 75.4% missing)\n",
      "   32. basic_needs                         [float64   ] 1,100 values ( 75.4% missing)\n",
      "   33. mental_health_index                 [float64   ] 1,100 values ( 75.4% missing)\n",
      "   34. protective_factors_score            [float64   ] 1,100 values ( 75.4% missing)\n",
      "   35. stress_risk_score                   [float64   ] 1,100 values ( 75.4% missing)\n",
      "   36. environmental_quality_score         [float64   ] 1,100 values ( 75.4% missing)\n",
      "   37. physical_symptoms_score             [float64   ] 1,100 values ( 75.4% missing)\n",
      "   38. social_support                      [float64   ] 1,100 values ( 75.4% missing)\n"
     ]
    }
   ],
   "source": [
    "# 2. Column Overview\n",
    "print(f\"\\n2. COLUMN OVERVIEW:\")\n",
    "print(f\"   All columns ({len(df_unified.columns)}):\")\n",
    "for i, col in enumerate(df_unified.columns, 1):\n",
    "    dtype = str(df_unified[col].dtype)\n",
    "    null_count = df_unified[col].isnull().sum()\n",
    "    null_pct = (null_count / len(df_unified)) * 100\n",
    "    non_null_count = df_unified[col].notna().sum()\n",
    "    \n",
    "    # Determine completeness status\n",
    "    if null_count == 0:\n",
    "        status = \"Complete\"\n",
    "    elif null_pct < 50:\n",
    "        status = f\" {null_pct:.1f}% missing\"\n",
    "    else:\n",
    "        status = f\" {null_pct:.1f}% missing\"\n",
    "    \n",
    "    print(f\"   {i:2d}. {col:35s} [{dtype:10s}] {non_null_count:5,} values ({status})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63393ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. MISSING VALUES PATTERN ANALYSIS:\n",
      "   Columns with missing values: 33\n",
      "\n",
      "   Missing values by column (sorted):\n",
      "      sleep_duration                     : 4,100 (91.64%) - Expected in: Dataset 01 only\n",
      "      Occupation                         : 4,100 (91.64%) - Expected in: Dataset 01 only\n",
      "      sleep_quality                      : 4,100 (91.64%) - Expected in: Dataset 01 only\n",
      "      physical_activity                  : 4,100 (91.64%) - Expected in: Dataset 01 only\n",
      "      heart_rate                         : 4,100 (91.64%) - Expected in: Dataset 01 only\n",
      "      bmi_category                       : 4,100 (91.64%) - Expected in: Dataset 01 only\n",
      "      stress_category                    : 4,100 (91.64%) - Expected in: Dataset 01 only\n",
      "      sleep_quality_category             : 4,100 (91.64%) - Expected in: Dataset 01 only\n",
      "      blood_pressure_diastolic           : 4,100 (91.64%) - Expected in: Dataset 01 only\n",
      "      environmental_quality_score        : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      stress_risk_score                  : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      mental_health_index                : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      basic_needs                        : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      safety                             : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      living_conditions                  : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      noise_level                        : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      breathing_problem                  : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      headache                           : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      mental_health_history              : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      depression                         : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      self_esteem                        : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      anxiety_level                      : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      social_support                     : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      protective_factors_score           : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      physical_symptoms_score            : 3,374 (75.41%) - Expected in: Dataset 04 only\n",
      "      happiness_score                    : 1,474 (32.95%) - Expected in: Dataset 03 only\n",
      "      work_hours                         : 1,474 (32.95%) - Expected in: Dataset 03 only\n",
      "      exercise_level                     : 1,474 (32.95%) - Expected in: Dataset 03 only\n",
      "      sleep_hours                        : 1,474 (32.95%) - Expected in: Dataset 03 only\n",
      "      diet_type                          : 1,474 (32.95%) - Expected in: Dataset 03 only\n",
      "      screen_time                        : 1,474 (32.95%) - Expected in: Dataset 03 only\n",
      "      social_interaction                 : 1,474 (32.95%) - Expected in: Dataset 03 only\n",
      "      stress_level                       : 1,100 (24.59%) - Expected in: Datasets 01 & 03\n"
     ]
    }
   ],
   "source": [
    "# 3. Missing Values Pattern Analysis\n",
    "print(f\"\\n3. MISSING VALUES PATTERN ANALYSIS:\")\n",
    "missing_by_column = df_unified.isnull().sum().sort_values(ascending=False)\n",
    "missing_by_column = missing_by_column[missing_by_column > 0]\n",
    "\n",
    "if len(missing_by_column) > 0:\n",
    "    print(f\"   Columns with missing values: {len(missing_by_column)}\")\n",
    "    print(f\"\\n   Missing values by column (sorted):\")\n",
    "    for col, count in missing_by_column.items():\n",
    "        pct = (count / len(df_unified)) * 100\n",
    "        # Determine which dataset should have this column\n",
    "        if col in df_sleep_final.columns and col not in df_lifestyle_final.columns and col not in df_stress_final.columns:\n",
    "            expected_in = \"Dataset 01 only\"\n",
    "        elif col in df_lifestyle_final.columns and col not in df_sleep_final.columns and col not in df_stress_final.columns:\n",
    "            expected_in = \"Dataset 03 only\"\n",
    "        elif col in df_stress_final.columns and col not in df_sleep_final.columns and col not in df_lifestyle_final.columns:\n",
    "            expected_in = \"Dataset 04 only\"\n",
    "        elif col in df_sleep_final.columns and col in df_lifestyle_final.columns:\n",
    "            expected_in = \"Datasets 01 & 03\"\n",
    "        elif col in df_sleep_final.columns and col in df_stress_final.columns:\n",
    "            expected_in = \"Datasets 01 & 04\"\n",
    "        elif col in df_lifestyle_final.columns and col in df_stress_final.columns:\n",
    "            expected_in = \"Datasets 03 & 04\"\n",
    "        else:\n",
    "            expected_in = \"Multiple datasets\"\n",
    "        \n",
    "        print(f\"      {col:35s}: {count:5,} ({pct:5.2f}%) - Expected in: {expected_in}\")\n",
    "else:\n",
    "    print(\"    No missing values!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ddbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. COMMON VARIABLES VERIFICATION:\n",
      "   ‚úì age                      : int64      - Complete (0 missing)\n",
      "   ‚úì gender                   : object     - Complete (0 missing)\n",
      "      Values: {'Female': 1759, 'Male': 1719, 'Other': 996}\n",
      "   ‚úì sleep_quality_norm       : float64    - Complete (0 missing)\n",
      "   ‚úì stress_level_norm        : int64      - Complete (0 missing)\n",
      "      Distribution: {0: 1452, 1: 1581, 2: 1441}\n",
      "   ‚úì dataset_source           : object     - Complete (0 missing)\n",
      "      Distribution: {'03_mental_health': 3000, '04_stress_level': 1100, '01_sleep_health': 374}\n"
     ]
    }
   ],
   "source": [
    "# 4. Common Variables Verification\n",
    "print(f\"\\n4. COMMON VARIABLES VERIFICATION:\")\n",
    "common_vars = ['age', 'gender', 'sleep_quality_norm', 'stress_level_norm', 'dataset_source']\n",
    "\n",
    "for var in common_vars:\n",
    "    if var in df_unified.columns:\n",
    "        null_count = df_unified[var].isnull().sum()\n",
    "        dtype = str(df_unified[var].dtype)\n",
    "        \n",
    "        if null_count == 0:\n",
    "            print(f\"   ‚úì {var:25s}: {dtype:10s} - Complete (0 missing)\")\n",
    "        else:\n",
    "            pct = (null_count / len(df_unified)) * 100\n",
    "            print(f\"    {var:25s}: {dtype:10s} - {null_count:,} missing ({pct:.2f}%)\")\n",
    "            \n",
    "        # Check distribution if categorical or numeric with limited values\n",
    "        if var == 'gender':\n",
    "            print(f\"      Values: {df_unified[var].value_counts().to_dict()}\")\n",
    "        elif var == 'dataset_source':\n",
    "            print(f\"      Distribution: {df_unified[var].value_counts().to_dict()}\")\n",
    "        elif var in ['stress_level_norm']:\n",
    "            print(f\"      Distribution: {df_unified[var].value_counts().sort_index().to_dict()}\")\n",
    "    else:\n",
    "        print(f\"   {var:25s}: NOT FOUND!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "86c8058f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. DATA TYPE CONSISTENCY CHECK:\n",
      "   Checking common variables across all datasets...\n",
      "    age                      : Consistent type (int)\n",
      "    gender                   : Consistent type (str)\n",
      "    sleep_quality_norm       : Consistent type (float)\n",
      "    stress_level_norm        : Consistent type (int)\n"
     ]
    }
   ],
   "source": [
    "# 5. Data Type Consistency Check\n",
    "print(f\"\\n5. DATA TYPE CONSISTENCY CHECK:\")\n",
    "print(f\"   Checking common variables across all datasets...\")\n",
    "\n",
    "common_vars_check = ['age', 'gender', 'sleep_quality_norm', 'stress_level_norm']\n",
    "for var in common_vars_check:\n",
    "    if var in df_unified.columns:\n",
    "        # Check if data type is consistent across all rows (should be, but verify)\n",
    "        unique_dtypes = df_unified[var].apply(type).unique()\n",
    "        if len(unique_dtypes) == 1:\n",
    "            print(f\"    {var:25s}: Consistent type ({unique_dtypes[0].__name__})\")\n",
    "        else:\n",
    "            print(f\"    {var:25s}: Multiple types detected {unique_dtypes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97d1ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. DATASET SOURCE VERIFICATION:\n",
      "   Distribution:\n",
      "      Correct 01_sleep_health     :   374 rows (expected: 374)\n",
      "      Correct 03_mental_health    : 3,000 rows (expected: 3000)\n",
      "      Correct 04_stress_level     : 1,100 rows (expected: 1100)\n"
     ]
    }
   ],
   "source": [
    "# 6. Dataset Source Verification\n",
    "print(f\"\\n6. DATASET SOURCE VERIFICATION:\")\n",
    "if 'dataset_source' in df_unified.columns:\n",
    "    source_dist = df_unified['dataset_source'].value_counts().sort_index()\n",
    "    print(f\"   Distribution:\")\n",
    "    for source, count in source_dist.items():\n",
    "        expected_count = {\n",
    "            '01_sleep_health': 374,\n",
    "            '03_mental_health': 3000,\n",
    "            '04_stress_level': 1100\n",
    "        }.get(source, 'Unknown')\n",
    "        status = \"Correct\" if count == expected_count else \"Dangerous\"\n",
    "        print(f\"      {status} {source:20s}: {count:5,} rows (expected: {expected_count})\")\n",
    "else:\n",
    "    print(\"   'dataset_source' column not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "34cea8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. TARGET VARIABLE CHECK (stress_level_norm):\n",
      "   Summary Statistics:\n",
      "      Count: 4,474\n",
      "      Mean: 0.998\n",
      "      Std: 0.804\n",
      "      Min: 0\n",
      "      Max: 2\n",
      "\n",
      "   Distribution:\n",
      "      0: 1,452 (32.45%)\n",
      "      1: 1,581 (35.34%)\n",
      "      2: 1,441 (32.21%)\n",
      "\n",
      "   Distribution by Dataset Source:\n",
      "stress_level_norm     0     1     2   All\n",
      "dataset_source                           \n",
      "01_sleep_health      71   233    70   374\n",
      "03_mental_health   1008   990  1002  3000\n",
      "04_stress_level     373   358   369  1100\n",
      "All                1452  1581  1441  4474\n"
     ]
    }
   ],
   "source": [
    "# 7. Target Variable Check\n",
    "print(f\"\\n7. TARGET VARIABLE CHECK (stress_level_norm):\")\n",
    "if 'stress_level_norm' in df_unified.columns:\n",
    "    target_info = df_unified['stress_level_norm'].describe()\n",
    "    print(f\"   Summary Statistics:\")\n",
    "    print(f\"      Count: {target_info['count']:,.0f}\")\n",
    "    print(f\"      Mean: {target_info['mean']:.3f}\")\n",
    "    print(f\"      Std: {target_info['std']:.3f}\")\n",
    "    print(f\"      Min: {target_info['min']:.0f}\")\n",
    "    print(f\"      Max: {target_info['max']:.0f}\")\n",
    "    print(f\"\\n   Distribution:\")\n",
    "    target_dist = df_unified['stress_level_norm'].value_counts().sort_index()\n",
    "    for val, count in target_dist.items():\n",
    "        pct = (count / len(df_unified)) * 100\n",
    "        print(f\"      {val}: {count:5,} ({pct:5.2f}%)\")\n",
    "    \n",
    "    # Check distribution by dataset source\n",
    "    print(f\"\\n   Distribution by Dataset Source:\")\n",
    "    target_by_source = pd.crosstab(df_unified['dataset_source'], df_unified['stress_level_norm'], margins=True)\n",
    "    print(target_by_source)\n",
    "else:\n",
    "    print(\"   ‚úó'stress_level_norm' not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2553473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. POTENTIAL ISSUES SUMMARY:\n",
      "    Issues found: 1\n",
      "      ‚Ä¢ Columns with >90% missing: 9\n"
     ]
    }
   ],
   "source": [
    "# 8. Potential Issues Summary\n",
    "print(f\"\\n8. POTENTIAL ISSUES SUMMARY:\")\n",
    "issues = []\n",
    "\n",
    "# Check for completely empty columns\n",
    "empty_cols = df_unified.columns[df_unified.isnull().all()].tolist()\n",
    "if empty_cols:\n",
    "    issues.append(f\"Empty columns found: {len(empty_cols)} ({', '.join(empty_cols)})\")\n",
    "\n",
    "# Check for columns with >90% missing\n",
    "high_missing = missing_by_column[missing_by_column > len(df_unified) * 0.9]\n",
    "if len(high_missing) > 0:\n",
    "    issues.append(f\"Columns with >90% missing: {len(high_missing)}\")\n",
    "\n",
    "# Check target variable\n",
    "if 'stress_level_norm' in df_unified.columns:\n",
    "    target_nulls = df_unified['stress_level_norm'].isnull().sum()\n",
    "    if target_nulls > 0:\n",
    "        issues.append(f\"Target variable has {target_nulls} missing values!\")\n",
    "\n",
    "if issues:\n",
    "    print(f\"    Issues found: {len(issues)}\")\n",
    "    for issue in issues:\n",
    "        print(f\"      ‚Ä¢ {issue}\")\n",
    "else:\n",
    "    print(f\"    No major issues detected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d6314f",
   "metadata": {},
   "source": [
    "## 9. Post-Merge Validation\n",
    "\n",
    "### Objective\n",
    "Perform comprehensive validation of the unified dataset to ensure:\n",
    "- **Data Integrity**: All records preserved correctly\n",
    "- **Data Quality**: Missing values patterns are expected and acceptable\n",
    "- **Target Variable**: Properly distributed across all dataset sources\n",
    "- **Feature Completeness**: All variables are properly merged\n",
    "- **Model Readiness**: Dataset is ready for machine learning pipeline\n",
    "\n",
    "### Validation Checklist\n",
    "1. ‚úÖ Row count verification\n",
    "2. ‚úÖ Column completeness analysis\n",
    "3. ‚úÖ Missing values pattern verification\n",
    "4. ‚úÖ Target variable distribution validation\n",
    "5. ‚úÖ Data type consistency check\n",
    "6. ‚úÖ Statistical summary of key variables\n",
    "7. ‚úÖ Dataset source balance verification\n",
    "8. ‚úÖ Feature quality assessment for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e595553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ROW COUNT VERIFICATION\n",
      "----------------------------------------------------------------------\n",
      "   Expected total rows: 4,474\n",
      "   Actual total rows:   4,474\n",
      "    Status: PASSED - All rows preserved correctly\n",
      "\n",
      "   Breakdown by dataset source:\n",
      "      Passed 01_sleep_health     :   374 (expected: 374)\n",
      "      Passed 03_mental_health    : 3,000 (expected: 3000)\n",
      "      Passed 04_stress_level     : 1,100 (expected: 1100)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. ROW COUNT VERIFICATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "expected_rows = len(df_sleep_final) + len(df_lifestyle_final) + len(df_stress_final)\n",
    "actual_rows = len(df_unified)\n",
    "\n",
    "print(f\"   Expected total rows: {expected_rows:,}\")\n",
    "print(f\"   Actual total rows:   {actual_rows:,}\")\n",
    "\n",
    "if expected_rows == actual_rows:\n",
    "    print(\"    Status: PASSED - All rows preserved correctly\")\n",
    "else:\n",
    "    diff = abs(expected_rows - actual_rows)\n",
    "    print(f\"   Status: FAILED - Row count mismatch: {diff} rows difference\")\n",
    "    print(f\"   Investigation required!\")\n",
    "\n",
    "# Breakdown by dataset source\n",
    "print(f\"\\n   Breakdown by dataset source:\")\n",
    "for source in ['01_sleep_health', '03_mental_health', '04_stress_level']:\n",
    "    count = len(df_unified[df_unified['dataset_source'] == source])\n",
    "    expected = {\n",
    "        '01_sleep_health': 374,\n",
    "        '03_mental_health': 3000,\n",
    "        '04_stress_level': 1100\n",
    "    }[source]\n",
    "    status = \"Passed\" if count == expected else \"Failed\"\n",
    "    print(f\"      {status} {source:20s}: {count:5,} (expected: {expected})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b3c502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. COLUMN COMPLETENESS ANALYSIS\n",
      "----------------------------------------------------------------------\n",
      "   Total columns: 38\n",
      "   Columns with complete data: 5\n",
      "   Columns with missing values: 33\n",
      "\n",
      "   Column Categories:\n",
      "      Complete (100%):      5 columns\n",
      "      Partial (has nulls): 33 columns\n",
      "     Empty (100% nulls):    0 columns\n",
      "\n",
      "   Complete columns (critical for ML):\n",
      "      ‚Ä¢ age\n",
      "      ‚Ä¢ dataset_source\n",
      "      ‚Ä¢ gender\n",
      "      ‚Ä¢ sleep_quality_norm\n",
      "      ‚Ä¢ stress_level_norm\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. COLUMN COMPLETENESS ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "total_columns = len(df_unified.columns)\n",
    "columns_with_complete_data = len(df_unified.columns[df_unified.notna().all()])\n",
    "columns_with_missing = len(df_unified.columns[df_unified.isnull().any()])\n",
    "\n",
    "print(f\"   Total columns: {total_columns}\")\n",
    "print(f\"   Columns with complete data: {columns_with_complete_data}\")\n",
    "print(f\"   Columns with missing values: {columns_with_missing}\")\n",
    "\n",
    "# Categorize columns by completeness\n",
    "complete_cols = [col for col in df_unified.columns if df_unified[col].notna().all()]\n",
    "partial_cols = [col for col in df_unified.columns if df_unified[col].isnull().any() and df_unified[col].notna().any()]\n",
    "empty_cols = [col for col in df_unified.columns if df_unified[col].isnull().all()]\n",
    "\n",
    "print(f\"\\n   Column Categories:\")\n",
    "print(f\"      Complete (100%):     {len(complete_cols):2d} columns\")\n",
    "print(f\"      Partial (has nulls): {len(partial_cols):2d} columns\")\n",
    "print(f\"     Empty (100% nulls):   {len(empty_cols):2d} columns\")\n",
    "\n",
    "if len(complete_cols) > 0:\n",
    "    print(f\"\\n   Complete columns (critical for ML):\")\n",
    "    for col in sorted(complete_cols):\n",
    "        print(f\"      ‚Ä¢ {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74d7e86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. MISSING VALUES PATTERN VERIFICATION\n",
      "----------------------------------------------------------------------\n",
      "   Total missing values: 102,302\n",
      "   Missing percentage: 60.17%\n",
      "\n",
      "   Missing Values by Dataset Source (Expected Pattern):\n",
      "   (Missing values are expected due to vertical merge)\n",
      "      01_sleep_health     :  8,602 missing (60.53%) from 374 rows\n",
      "      03_mental_health    : 75,000 missing (65.79%) from 3,000 rows\n",
      "      04_stress_level     : 18,700 missing (44.74%) from 1,100 rows\n",
      "\n",
      "   Critical Variables Missing Check:\n",
      "      age                      : 0 missing\n",
      "      gender                   : 0 missing\n",
      "      sleep_quality_norm       : 0 missing\n",
      "      stress_level_norm        : 0 missing\n",
      "      dataset_source           : 0 missing\n",
      "\n",
      "   All critical variables are complete - Dataset is valid for ML\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3. MISSING VALUES PATTERN VERIFICATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"   Total missing values: {df_unified.isnull().sum().sum():,}\")\n",
    "print(f\"   Missing percentage: {(df_unified.isnull().sum().sum() / (len(df_unified) * len(df_unified.columns))) * 100:.2f}%\")\n",
    "\n",
    "# Analyze missing patterns by dataset source\n",
    "print(f\"\\n   Missing Values by Dataset Source (Expected Pattern):\")\n",
    "print(f\"   (Missing values are expected due to vertical merge)\")\n",
    "\n",
    "missing_by_source = {}\n",
    "for source in df_unified['dataset_source'].unique():\n",
    "    source_data = df_unified[df_unified['dataset_source'] == source]\n",
    "    missing_count = source_data.isnull().sum().sum()\n",
    "    total_cells = len(source_data) * len(source_data.columns)\n",
    "    missing_pct = (missing_count / total_cells) * 100\n",
    "    missing_by_source[source] = {\n",
    "        'count': missing_count,\n",
    "        'pct': missing_pct,\n",
    "        'rows': len(source_data)\n",
    "    }\n",
    "    print(f\"      {source:20s}: {missing_count:6,} missing ({missing_pct:5.2f}%) from {len(source_data):,} rows\")\n",
    "\n",
    "# Verify that common variables have no missing values\n",
    "print(f\"\\n   Critical Variables Missing Check:\")\n",
    "critical_vars = ['age', 'gender', 'sleep_quality_norm', 'stress_level_norm', 'dataset_source']\n",
    "all_complete = True\n",
    "for var in critical_vars:\n",
    "    if var in df_unified.columns:\n",
    "        missing = df_unified[var].isnull().sum()\n",
    "        if missing == 0:\n",
    "            print(f\"      {var:25s}: 0 missing\")\n",
    "        else:\n",
    "            print(f\"       {var:25s}: {missing:,} missing - CRITICAL ISSUE!\")\n",
    "            all_complete = False\n",
    "    else:\n",
    "        print(f\"       {var:25s}: COLUMN NOT FOUND!\")\n",
    "        all_complete = False\n",
    "\n",
    "if all_complete:\n",
    "    print(f\"\\n   All critical variables are complete - Dataset is valid for ML\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c81d8091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. TARGET VARIABLE DISTRIBUTION VALIDATION\n",
      "----------------------------------------------------------------------\n",
      "   Overall Distribution:\n",
      "      0 (Low   ): 1,452 (32.45%)\n",
      "      1 (Medium): 1,581 (35.34%)\n",
      "      2 (High  ): 1,441 (32.21%)\n",
      "\n",
      "   Status: WELL BALANCED (max deviation: 2.00%)\n",
      "\n",
      "   Distribution by Dataset Source:\n",
      "stress_level_norm     0    1     2\n",
      "dataset_source                    \n",
      "01_sleep_health      71  233    70\n",
      "03_mental_health   1008  990  1002\n",
      "04_stress_level     373  358   369\n",
      "\n",
      "   Target Class Coverage by Source:\n",
      "      01_sleep_health     : All classes present [np.int64(0), np.int64(1), np.int64(2)]\n",
      "      03_mental_health    : All classes present [np.int64(0), np.int64(1), np.int64(2)]\n",
      "      04_stress_level     : All classes present [np.int64(0), np.int64(1), np.int64(2)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4. TARGET VARIABLE DISTRIBUTION VALIDATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "target_var = 'stress_level_norm'\n",
    "\n",
    "if target_var in df_unified.columns:\n",
    "    # Overall distribution\n",
    "    target_dist = df_unified[target_var].value_counts().sort_index()\n",
    "    print(f\"   Overall Distribution:\")\n",
    "    for val, count in target_dist.items():\n",
    "        pct = (count / len(df_unified)) * 100\n",
    "        label = {0: \"Low\", 1: \"Medium\", 2: \"High\"}.get(val, \"Unknown\")\n",
    "        print(f\"      {val} ({label:6s}): {count:5,} ({pct:5.2f}%)\")\n",
    "    \n",
    "    # Check balance (ideal: 33.33% each for balanced classification)\n",
    "    balance_check = target_dist / len(df_unified)\n",
    "    max_imbalance = abs(balance_check - (1/3)).max()\n",
    "    \n",
    "    if max_imbalance < 0.05:  # Within 5% of perfect balance\n",
    "        print(f\"\\n   Status: WELL BALANCED (max deviation: {max_imbalance*100:.2f}%)\")\n",
    "    elif max_imbalance < 0.10:  # Within 10% of perfect balance\n",
    "        print(f\"\\n    Status: MODERATELY BALANCED (max deviation: {max_imbalance*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n   Status: IMBALANCED (max deviation: {max_imbalance*100:.2f}%)\")\n",
    "        print(f\"      Recommendation: Consider class weights in ML model\")\n",
    "    \n",
    "    # Distribution by dataset source\n",
    "    print(f\"\\n   Distribution by Dataset Source:\")\n",
    "    target_by_source = pd.crosstab(df_unified['dataset_source'], df_unified[target_var])\n",
    "    print(target_by_source)\n",
    "    \n",
    "    # Verify each source has all target classes\n",
    "    print(f\"\\n   Target Class Coverage by Source:\")\n",
    "    for source in df_unified['dataset_source'].unique():\n",
    "        source_classes = df_unified[df_unified['dataset_source'] == source][target_var].unique()\n",
    "        all_classes = [0, 1, 2]\n",
    "        missing_classes = set(all_classes) - set(source_classes)\n",
    "        if len(missing_classes) == 0:\n",
    "            print(f\"      {source:20s}: All classes present {sorted(source_classes)}\")\n",
    "        else:\n",
    "            print(f\"      {source:20s}: Missing classes {sorted(missing_classes)}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"   Target variable '{target_var}' not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9de2556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. DATA TYPE CONSISTENCY CHECK\n",
      "----------------------------------------------------------------------\n",
      "   Checking data types across the unified dataset:\n",
      "\n",
      "   Numeric Variables:\n",
      "      age                                : int64      (    0 nulls)\n",
      "      anxiety_level                      : float64    (3,374 nulls)\n",
      "      basic_needs                        : float64    (3,374 nulls)\n",
      "      blood_pressure_diastolic           : float64    (4,100 nulls)\n",
      "      breathing_problem                  : float64    (3,374 nulls)\n",
      "      depression                         : float64    (3,374 nulls)\n",
      "      environmental_quality_score        : float64    (3,374 nulls)\n",
      "      happiness_score                    : float64    (1,474 nulls)\n",
      "      headache                           : float64    (3,374 nulls)\n",
      "      heart_rate                         : float64    (4,100 nulls)\n",
      "      living_conditions                  : float64    (3,374 nulls)\n",
      "      mental_health_history              : float64    (3,374 nulls)\n",
      "      mental_health_index                : float64    (3,374 nulls)\n",
      "      noise_level                        : float64    (3,374 nulls)\n",
      "      physical_activity                  : float64    (4,100 nulls)\n",
      "      physical_symptoms_score            : float64    (3,374 nulls)\n",
      "      protective_factors_score           : float64    (3,374 nulls)\n",
      "      safety                             : float64    (3,374 nulls)\n",
      "      screen_time                        : float64    (1,474 nulls)\n",
      "      self_esteem                        : float64    (3,374 nulls)\n",
      "      sleep_duration                     : float64    (4,100 nulls)\n",
      "      sleep_hours                        : float64    (1,474 nulls)\n",
      "      sleep_quality                      : float64    (4,100 nulls)\n",
      "      sleep_quality_norm                 : float64    (    0 nulls)\n",
      "      social_interaction                 : float64    (1,474 nulls)\n",
      "      social_support                     : float64    (3,374 nulls)\n",
      "      stress_level_norm                  : int64      (    0 nulls)\n",
      "      stress_risk_score                  : float64    (3,374 nulls)\n",
      "      work_hours                         : float64    (1,474 nulls)\n",
      "\n",
      "   Categorical Variables:\n",
      "      Occupation                         : object     (4,100 nulls,  11 unique values)\n",
      "      bmi_category                       : object     (4,100 nulls,   4 unique values)\n",
      "      dataset_source                     : object     (    0 nulls,   3 unique values)\n",
      "      diet_type                          : object     (1,474 nulls,   5 unique values)\n",
      "      exercise_level                     : object     (1,474 nulls,   3 unique values)\n",
      "      gender                             : object     (    0 nulls,   3 unique values)\n",
      "      sleep_quality_category             : object     (4,100 nulls,   3 unique values)\n",
      "      stress_category                    : object     (4,100 nulls,   3 unique values)\n",
      "      stress_level                       : object     (1,100 nulls,   9 unique values)\n",
      "\n",
      "   Data Type Consistency Status:\n",
      "       Mixed types detected in 1 columns:\n",
      "         ‚Ä¢ stress_level: [<class 'int'> <class 'str'>]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. DATA TYPE CONSISTENCY CHECK\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"   Checking data types across the unified dataset:\")\n",
    "print(f\"\\n   Numeric Variables:\")\n",
    "numeric_cols = df_unified.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for col in sorted(numeric_cols):\n",
    "    dtype = str(df_unified[col].dtype)\n",
    "    null_count = df_unified[col].isnull().sum()\n",
    "    print(f\"      {col:35s}: {dtype:10s} ({null_count:5,} nulls)\")\n",
    "\n",
    "print(f\"\\n   Categorical Variables:\")\n",
    "categorical_cols = df_unified.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in sorted(categorical_cols):\n",
    "    dtype = str(df_unified[col].dtype)\n",
    "    null_count = df_unified[col].isnull().sum()\n",
    "    unique_count = df_unified[col].nunique()\n",
    "    print(f\"      {col:35s}: {dtype:10s} ({null_count:5,} nulls, {unique_count:3,} unique values)\")\n",
    "\n",
    "# Check for mixed types (potential issues)\n",
    "print(f\"\\n   Data Type Consistency Status:\")\n",
    "type_issues = []\n",
    "for col in df_unified.columns:\n",
    "    # Check if column has mixed types (would indicate data quality issue)\n",
    "    if df_unified[col].dtype == 'object':\n",
    "        # For object columns, check if values are consistent\n",
    "        non_null_values = df_unified[col].dropna()\n",
    "        if len(non_null_values) > 0:\n",
    "            value_types = non_null_values.apply(type).unique()\n",
    "            if len(value_types) > 1:\n",
    "                type_issues.append((col, value_types))\n",
    "\n",
    "if len(type_issues) == 0:\n",
    "    print(f\"      No mixed type issues detected\")\n",
    "else:\n",
    "    print(f\"       Mixed types detected in {len(type_issues)} columns:\")\n",
    "    for col, types in type_issues:\n",
    "        print(f\"         ‚Ä¢ {col}: {types}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0fbbf6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. STATISTICAL SUMMARY OF KEY VARIABLES\n",
      "----------------------------------------------------------------------\n",
      "   Numeric Variables Summary:\n",
      "\n",
      "      age:\n",
      "         Count:    4,474\n",
      "         Mean:     36.934\n",
      "         Std:      13.774\n",
      "         Min:      18.00\n",
      "         25%:      25.00\n",
      "         50%:      35.00\n",
      "         75%:      49.00\n",
      "         Max:      64.00\n",
      "\n",
      "      sleep_quality_norm:\n",
      "         Count:    4,474\n",
      "         Mean:     0.561\n",
      "         Std:      0.203\n",
      "         Min:      0.00\n",
      "         25%:      0.43\n",
      "         50%:      0.56\n",
      "         75%:      0.67\n",
      "         Max:      1.00\n",
      "\n",
      "      stress_level_norm:\n",
      "         Count:    4,474\n",
      "         Mean:     0.998\n",
      "         Std:      0.804\n",
      "         Min:      0.00\n",
      "         25%:      0.00\n",
      "         50%:      1.00\n",
      "         75%:      2.00\n",
      "         Max:      2.00\n",
      "\n",
      "   Categorical Variables Summary:\n",
      "\n",
      "      gender:\n",
      "         Female              : 1,759 (39.32%)\n",
      "         Male                : 1,719 (38.42%)\n",
      "         Other               :   996 (22.26%)\n",
      "\n",
      "      dataset_source:\n",
      "         03_mental_health    : 3,000 (67.05%)\n",
      "         04_stress_level     : 1,100 (24.59%)\n",
      "         01_sleep_health     :   374 ( 8.36%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n6. STATISTICAL SUMMARY OF KEY VARIABLES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "key_numeric_vars = ['age', 'sleep_quality_norm', 'stress_level_norm']\n",
    "key_categorical_vars = ['gender', 'dataset_source']\n",
    "\n",
    "print(f\"   Numeric Variables Summary:\")\n",
    "for var in key_numeric_vars:\n",
    "    if var in df_unified.columns:\n",
    "        stats = df_unified[var].describe()\n",
    "        print(f\"\\n      {var}:\")\n",
    "        print(f\"         Count:    {stats['count']:,.0f}\")\n",
    "        print(f\"         Mean:     {stats['mean']:.3f}\")\n",
    "        print(f\"         Std:      {stats['std']:.3f}\")\n",
    "        print(f\"         Min:      {stats['min']:.2f}\")\n",
    "        print(f\"         25%:      {stats['25%']:.2f}\")\n",
    "        print(f\"         50%:      {stats['50%']:.2f}\")\n",
    "        print(f\"         75%:      {stats['75%']:.2f}\")\n",
    "        print(f\"         Max:      {stats['max']:.2f}\")\n",
    "\n",
    "print(f\"\\n   Categorical Variables Summary:\")\n",
    "for var in key_categorical_vars:\n",
    "    if var in df_unified.columns:\n",
    "        value_counts = df_unified[var].value_counts()\n",
    "        print(f\"\\n      {var}:\")\n",
    "        for val, count in value_counts.items():\n",
    "            pct = (count / len(df_unified)) * 100\n",
    "            print(f\"         {str(val):20s}: {count:5,} ({pct:5.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c15c098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. DATASET SOURCE BALANCE VERIFICATION\n",
      "----------------------------------------------------------------------\n",
      "   Source Distribution:\n",
      "      01_sleep_health     :   374 ( 8.36%)\n",
      "      03_mental_health    : 3,000 (67.05%)\n",
      "      04_stress_level     : 1,100 (24.59%)\n",
      "\n",
      "   Status: HIGHLY IMBALANCED (imbalance: 58.7%)\n",
      "      Recommendation: Consider stratified sampling if needed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n7. DATASET SOURCE BALANCE VERIFICATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "source_dist = df_unified['dataset_source'].value_counts().sort_index()\n",
    "total = len(df_unified)\n",
    "\n",
    "print(f\"   Source Distribution:\")\n",
    "for source, count in source_dist.items():\n",
    "    pct = (count / total) * 100\n",
    "    print(f\"      {source:20s}: {count:5,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Check if distribution is acceptable\n",
    "max_pct = (source_dist.max() / total) * 100\n",
    "min_pct = (source_dist.min() / total) * 100\n",
    "imbalance = max_pct - min_pct\n",
    "\n",
    "if imbalance < 20:\n",
    "    print(f\"\\n   Status: WELL BALANCED (imbalance: {imbalance:.1f}%)\")\n",
    "elif imbalance < 40:\n",
    "    print(f\"\\n   Status: MODERATELY IMBALANCED (imbalance: {imbalance:.1f}%)\")\n",
    "    print(f\"      Note: Dataset 03 dominates, which is acceptable for this use case\")\n",
    "else:\n",
    "    print(f\"\\n   Status: HIGHLY IMBALANCED (imbalance: {imbalance:.1f}%)\")\n",
    "    print(f\"      Recommendation: Consider stratified sampling if needed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ea49686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. FEATURE QUALITY ASSESSMENT FOR ML\n",
      "----------------------------------------------------------------------\n",
      "   Feature Categories for Machine Learning:\n",
      "\n",
      "   Complete Features (4): Ready for ML\n",
      "      These features have no missing values and can be used directly:\n",
      "          1. age\n",
      "          2. gender\n",
      "          3. sleep_quality_norm\n",
      "          4. stress_level_norm\n",
      "\n",
      "   Partial Features (33): Need handling strategy\n",
      "      These features have missing values - options:\n",
      "         1. Imputation (mean, median, mode, or ML-based)\n",
      "         2. Drop columns if >50% missing (not recommended for this dataset)\n",
      "         3. Create indicator variables for missingness\n",
      "\n",
      "      Missing value analysis for top partial features:\n",
      "         ‚Ä¢ Occupation                         : 91.64% missing\n",
      "         ‚Ä¢ sleep_duration                     : 91.64% missing\n",
      "         ‚Ä¢ sleep_quality                      : 91.64% missing\n",
      "         ‚Ä¢ physical_activity                  : 91.64% missing\n",
      "         ‚Ä¢ bmi_category                       : 91.64% missing\n",
      "         ‚Ä¢ heart_rate                         : 91.64% missing\n",
      "         ‚Ä¢ blood_pressure_diastolic           : 91.64% missing\n",
      "         ‚Ä¢ sleep_quality_category             : 91.64% missing\n",
      "         ‚Ä¢ stress_category                    : 91.64% missing\n",
      "         ‚Ä¢ anxiety_level                      : 75.41% missing\n",
      "\n",
      "   Features to Exclude (1): Metadata\n",
      "      ‚Ä¢ dataset_source (used for tracking, not for prediction)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n8. FEATURE QUALITY ASSESSMENT FOR ML\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"   Feature Categories for Machine Learning:\")\n",
    "\n",
    "# Complete features (can be used directly)\n",
    "complete_features = [col for col in df_unified.columns \n",
    "                     if df_unified[col].notna().all() \n",
    "                     and col not in ['dataset_source']]\n",
    "print(f\"\\n   Complete Features ({len(complete_features)}): Ready for ML\")\n",
    "print(f\"      These features have no missing values and can be used directly:\")\n",
    "for i, feat in enumerate(sorted(complete_features), 1):\n",
    "    print(f\"         {i:2d}. {feat}\")\n",
    "\n",
    "# Features with missing values (need imputation or handling)\n",
    "partial_features = [col for col in df_unified.columns \n",
    "                    if df_unified[col].isnull().any() \n",
    "                    and df_unified[col].notna().any()\n",
    "                    and col != 'dataset_source']\n",
    "print(f\"\\n   Partial Features ({len(partial_features)}): Need handling strategy\")\n",
    "print(f\"      These features have missing values - options:\")\n",
    "print(f\"         1. Imputation (mean, median, mode, or ML-based)\")\n",
    "print(f\"         2. Drop columns if >50% missing (not recommended for this dataset)\")\n",
    "print(f\"         3. Create indicator variables for missingness\")\n",
    "\n",
    "# Calculate missing percentage for partial features\n",
    "if len(partial_features) > 0:\n",
    "    print(f\"\\n      Missing value analysis for top partial features:\")\n",
    "    missing_analysis = []\n",
    "    for feat in partial_features:\n",
    "        missing_pct = (df_unified[feat].isnull().sum() / len(df_unified)) * 100\n",
    "        missing_analysis.append((feat, missing_pct))\n",
    "    \n",
    "    missing_analysis.sort(key=lambda x: x[1], reverse=True)\n",
    "    for feat, pct in missing_analysis[:10]:  # Top 10\n",
    "        print(f\"         ‚Ä¢ {feat:35s}: {pct:5.2f}% missing\")\n",
    "\n",
    "# Features to exclude from ML\n",
    "exclude_features = ['dataset_source']  # Metadata, not a feature\n",
    "print(f\"\\n   Features to Exclude ({len(exclude_features)}): Metadata\")\n",
    "for feat in exclude_features:\n",
    "    print(f\"      ‚Ä¢ {feat} (used for tracking, not for prediction)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b24b877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL VALIDATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "   Validation Checklist:\n",
      "      PASSED      : Row Count\n",
      "      PASSED      : Critical Variables Complete\n",
      "      PASSED      : Target Variable Present\n",
      "      PASSED      : Target Variable Balanced\n",
      "      FAILED      : Data Types Consistent\n",
      "      PASSED      : No Empty Columns\n",
      "\n",
      "   Overall Status:\n",
      "      VALIDATION ISSUES DETECTED - Review required\n",
      "\n",
      "   Action Items:\n",
      "      ‚Ä¢ Fix: Data Types Consistent\n",
      "\n",
      "   Dataset Statistics:\n",
      "      Total records: 4,474\n",
      "      Total features: 37\n",
      "      Complete features: 4\n",
      "      Partial features: 33\n",
      "      Target classes: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "validation_results = {\n",
    "    'Row Count': expected_rows == actual_rows,\n",
    "    'Critical Variables Complete': all_complete,\n",
    "    'Target Variable Present': target_var in df_unified.columns,\n",
    "    'Target Variable Balanced': max_imbalance < 0.10 if target_var in df_unified.columns else False,\n",
    "    'Data Types Consistent': len(type_issues) == 0,\n",
    "    'No Empty Columns': len(empty_cols) == 0\n",
    "}\n",
    "\n",
    "print(f\"\\n   Validation Checklist:\")\n",
    "all_passed = True\n",
    "for check, passed in validation_results.items():\n",
    "    status = \"PASSED\" if passed else \"FAILED\"\n",
    "    print(f\"      {status:12s}: {check}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "print(f\"\\n   Overall Status:\")\n",
    "if all_passed:\n",
    "    print(f\"      DATASET IS VALIDATED AND READY FOR MODELING\")\n",
    "    print(f\"\\n   Next Steps:\")\n",
    "    print(f\"      1. Feature engineering (if needed)\")\n",
    "    print(f\"      2. Train/test split\")\n",
    "    print(f\"      3. Feature scaling/normalization\")\n",
    "    print(f\"      4. Handle missing values in partial features\")\n",
    "    print(f\"      5. Model training\")\n",
    "else:\n",
    "    print(f\"      VALIDATION ISSUES DETECTED - Review required\")\n",
    "    print(f\"\\n   Action Items:\")\n",
    "    for check, passed in validation_results.items():\n",
    "        if not passed:\n",
    "            print(f\"      ‚Ä¢ Fix: {check}\")\n",
    "\n",
    "print(f\"\\n   Dataset Statistics:\")\n",
    "print(f\"      Total records: {len(df_unified):,}\")\n",
    "print(f\"      Total features: {len(df_unified.columns) - 1}\")  # Exclude dataset_source\n",
    "print(f\"      Complete features: {len(complete_features)}\")\n",
    "print(f\"      Partial features: {len(partial_features)}\")\n",
    "print(f\"      Target classes: {df_unified[target_var].nunique() if target_var in df_unified.columns else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f19a5d",
   "metadata": {},
   "source": [
    "## 10. Generate Validation Report\n",
    "\n",
    "Save a comprehensive validation report documenting all findings and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c326b034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GENERATING VALIDATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Validation report saved!\n",
      "   JSON: c:\\Users\\rafae\\Desktop\\Personal_Information_App\\ai_personal_performance_coach\\datasets\\final\\validation_report.json\n",
      "   TXT:  c:\\Users\\rafae\\Desktop\\Personal_Information_App\\ai_personal_performance_coach\\datasets\\final\\validation_report.txt\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING VALIDATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create validation report dictionary\n",
    "validation_report = {\n",
    "    'validation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_info': {\n",
    "        'total_rows': len(df_unified),\n",
    "        'total_columns': len(df_unified.columns),\n",
    "        'total_cells': len(df_unified) * len(df_unified.columns),\n",
    "        'missing_values': int(df_unified.isnull().sum().sum()),\n",
    "        'missing_percentage': float((df_unified.isnull().sum().sum() / (len(df_unified) * len(df_unified.columns))) * 100)\n",
    "    },\n",
    "    'source_distribution': df_unified['dataset_source'].value_counts().to_dict(),\n",
    "    'target_distribution': df_unified['stress_level_norm'].value_counts().to_dict() if 'stress_level_norm' in df_unified.columns else {},\n",
    "    'complete_features': sorted(complete_features),\n",
    "    'partial_features': sorted(partial_features),\n",
    "    'validation_results': {k: bool(v) for k, v in validation_results.items()},\n",
    "    'data_quality_metrics': {\n",
    "        'critical_variables_complete': all_complete,\n",
    "        'target_variable_balanced': max_imbalance < 0.10 if 'stress_level_norm' in df_unified.columns else False,\n",
    "        'data_types_consistent': len(type_issues) == 0,\n",
    "        'no_empty_columns': len(empty_cols) == 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "report_path_json = FINAL_DATA_DIR / 'validation_report.json'\n",
    "with open(report_path_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(validation_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save as text report\n",
    "report_path_txt = FINAL_DATA_DIR / 'validation_report.txt'\n",
    "with open(report_path_txt, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"UNIFIED DATASET VALIDATION REPORT\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Validation Date: {validation_report['validation_date']}\\n\\n\")\n",
    "    \n",
    "    f.write(\"DATASET INFORMATION\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for key, value in validation_report['dataset_info'].items():\n",
    "        f.write(f\"{key.replace('_', ' ').title()}: {value:,}\\n\" if isinstance(value, (int, float)) else f\"{key.replace('_', ' ').title()}: {value}\\n\")\n",
    "    \n",
    "    f.write(\"\\nSOURCE DISTRIBUTION\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for source, count in validation_report['source_distribution'].items():\n",
    "        pct = (count / validation_report['dataset_info']['total_rows']) * 100\n",
    "        f.write(f\"{source:20s}: {count:5,} ({pct:5.2f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\nTARGET DISTRIBUTION\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for val, count in sorted(validation_report['target_distribution'].items()):\n",
    "        pct = (count / validation_report['dataset_info']['total_rows']) * 100\n",
    "        label = {0: \"Low\", 1: \"Medium\", 2: \"High\"}.get(val, \"Unknown\")\n",
    "        f.write(f\"{val} ({label:6s}): {count:5,} ({pct:5.2f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\nVALIDATION RESULTS\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for check, passed in validation_report['validation_results'].items():\n",
    "        status = \"PASSED\" if passed else \"FAILED\"\n",
    "        f.write(f\"{check:30s}: {status}\\n\")\n",
    "    \n",
    "    f.write(\"\\nDATA QUALITY METRICS\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for metric, value in validation_report['data_quality_metrics'].items():\n",
    "        status = \"YES\" if value else \"NO\"\n",
    "        f.write(f\"{metric.replace('_', ' ').title():30s}: {status}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nCOMPLETE FEATURES ({len(validation_report['complete_features'])})\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for i, feat in enumerate(validation_report['complete_features'], 1):\n",
    "        f.write(f\"{i:2d}. {feat}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nPARTIAL FEATURES ({len(validation_report['partial_features'])})\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for i, feat in enumerate(validation_report['partial_features'], 1):\n",
    "        missing_pct = (df_unified[feat].isnull().sum() / len(df_unified)) * 100\n",
    "        f.write(f\"{i:2d}. {feat:35s} ({missing_pct:5.2f}% missing)\\n\")\n",
    "\n",
    "print(f\"\\nValidation report saved!\")\n",
    "print(f\"   JSON: {report_path_json}\")\n",
    "print(f\"   TXT:  {report_path_txt}\")\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd930f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c91add9b",
   "metadata": {},
   "source": [
    "Now, We investigate the problem about datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b7802925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INVESTIGATING DATA TYPE CONSISTENCY ISSUE\n",
      "======================================================================\n",
      "\n",
      "Checking for mixed types in object columns...\n",
      "\n",
      " Found 1 columns with mixed types:\n",
      "\n",
      "   Column: stress_level\n",
      "   Types found: [\"<class 'int'>\", \"<class 'str'>\"]\n",
      "   Sample values: [6, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INVESTIGATING DATA TYPE CONSISTENCY ISSUE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nChecking for mixed types in object columns...\")\n",
    "\n",
    "type_issues_found = []\n",
    "for col in df_unified.columns:\n",
    "    if df_unified[col].dtype == 'object':\n",
    "        non_null_values = df_unified[col].dropna()\n",
    "        if len(non_null_values) > 0:\n",
    "            value_types = non_null_values.apply(type).unique()\n",
    "            if len(value_types) > 1:\n",
    "                type_issues_found.append({\n",
    "                    'column': col,\n",
    "                    'types': value_types,\n",
    "                    'sample_values': non_null_values.head(10).tolist()\n",
    "                })\n",
    "\n",
    "if len(type_issues_found) > 0:\n",
    "    print(f\"\\n Found {len(type_issues_found)} columns with mixed types:\")\n",
    "    for issue in type_issues_found:\n",
    "        print(f\"\\n   Column: {issue['column']}\")\n",
    "        print(f\"   Types found: {[str(t) for t in issue['types']]}\")\n",
    "        print(f\"   Sample values: {issue['sample_values'][:5]}\")\n",
    "else:\n",
    "    print(\"\\n No mixed types detected in object columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f3ba1adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Checking numeric columns consistency...\n",
      "All numeric columns are consistently numeric\n"
     ]
    }
   ],
   "source": [
    "# maintain consistency in data type\n",
    "# Additional check: verify all numeric columns are numeric\n",
    "print(\"\\n\\nChecking numeric columns consistency...\")\n",
    "numeric_cols = df_unified.select_dtypes(include=[np.number]).columns\n",
    "non_numeric_in_numeric = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    # Try to convert to numeric, see if there are any issues\n",
    "    try:\n",
    "        pd.to_numeric(df_unified[col], errors='raise')\n",
    "    except (ValueError, TypeError) as e:\n",
    "        non_numeric_in_numeric.append({\n",
    "            'column': col,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "if len(non_numeric_in_numeric) > 0:\n",
    "    print(f\"Found {len(non_numeric_in_numeric)} numeric columns with non-numeric values:\")\n",
    "    for issue in non_numeric_in_numeric:\n",
    "        print(f\"   {issue['column']}: {issue['error']}\")\n",
    "else:\n",
    "    print(\"All numeric columns are consistently numeric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "06a36f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Detailed column type analysis:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   gender:\n",
      "      dtype: object\n",
      "      unique value types: [\"<class 'str'>\"]\n",
      "      sample values: ['Male', 'Male', 'Male', 'Male', 'Male']\n",
      "       Consistent types\n",
      "\n",
      "   Occupation:\n",
      "      dtype: object\n",
      "      unique value types: [\"<class 'str'>\"]\n",
      "      sample values: ['Software Engineer', 'Doctor', 'Doctor', 'Sales Representative', 'Sales Representative']\n",
      "       Consistent types\n",
      "\n",
      "   bmi_category:\n",
      "      dtype: object\n",
      "      unique value types: [\"<class 'str'>\"]\n",
      "      sample values: ['Overweight', 'Normal', 'Normal', 'Obese', 'Obese']\n",
      "       Consistent types\n",
      "\n",
      "   stress_level:\n",
      "      dtype: object\n",
      "      unique value types: [\"<class 'int'>\", \"<class 'str'>\"]\n",
      "      sample values: [6, 8, 8, 8, 8]\n",
      "       WARNING: Mixed types detected!\n",
      "\n",
      "   exercise_level:\n",
      "      dtype: object\n",
      "      unique value types: [\"<class 'str'>\"]\n",
      "      sample values: ['Low', 'Moderate', 'Low', 'Low', 'Low']\n",
      "       Consistent types\n",
      "\n",
      "   diet_type:\n",
      "      dtype: object\n",
      "      unique value types: [\"<class 'str'>\"]\n",
      "      sample values: ['Vegetarian', 'Vegan', 'Vegetarian', 'Vegan', 'Balanced']\n",
      "       Consistent types\n"
     ]
    }
   ],
   "source": [
    "# Check other potential type issues \n",
    "print(\"\\n\\nDetailed column type analysis:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "suspicious_cols = ['gender', 'Occupation', 'bmi_category', 'stress_level', 'exercise_level', 'diet_type']\n",
    "for col in suspicious_cols:\n",
    "    if col in df_unified.columns:\n",
    "        dtype = df_unified[col].dtype\n",
    "        null_count = df_unified[col].isnull().sum()\n",
    "        non_null = df_unified[col].dropna()\n",
    "        \n",
    "        if len(non_null) > 0:\n",
    "            unique_types = non_null.apply(type).unique()\n",
    "            print(f\"\\n   {col}:\")\n",
    "            print(f\"      dtype: {dtype}\")\n",
    "            print(f\"      unique value types: {[str(t) for t in unique_types]}\")\n",
    "            print(f\"      sample values: {non_null.head(5).tolist()}\")\n",
    "            \n",
    "            if len(unique_types) > 1:\n",
    "                print(f\"       WARNING: Mixed types detected!\")\n",
    "            else:\n",
    "                print(f\"       Consistent types\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3038aebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Actual data type issues found: 1\n",
      "   These need to be addressed before modeling.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if len(type_issues_found) == 0 and len(non_numeric_in_numeric) == 0:\n",
    "    print(\"\\n No actual data type issues found!\")\n",
    "    print(\"   The 'Data Types Consistent: FAILED' might be a false positive.\")\n",
    "    print(\"   Recommendation: Review the validation logic in the post-merge validation code.\")\n",
    "else:\n",
    "    print(f\"\\n Actual data type issues found: {len(type_issues_found) + len(non_numeric_in_numeric)}\")\n",
    "    print(\"   These need to be addressed before modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55482040",
   "metadata": {},
   "source": [
    "## 11. Fix Data Type Consistency Issue\n",
    "\n",
    "### Problem\n",
    "The `stress_level` column has mixed types (int and string) due to different formats across datasets.\n",
    "\n",
    "### Solution\n",
    "Since we already have `stress_level_norm` (the normalized target variable), we have two options:\n",
    "1. **Remove the original `stress_level` column** (recommended) - It's redundant and we use `stress_level_norm` for modeling\n",
    "2. Standardize all values to a consistent format\n",
    "\n",
    "We'll proceed with Option 1 as it's cleaner and avoids confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1ad39c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Applying Fix:\n",
      "   Before: 38 columns\n",
      "   Statistics before removal:\n",
      "      Total non-null values: 3,374\n",
      "      Null values: 1,100\n",
      "      Unique types: [\"<class 'int'>\", \"<class 'str'>\"]\n",
      "   After: 37 columns\n",
      "   Column 'stress_level' removed successfully\n"
     ]
    }
   ],
   "source": [
    "# Check if stress_level exists and remove it\n",
    "if 'stress_level' in df_unified.columns:\n",
    "    print(f\"\\n4. Applying Fix:\")\n",
    "    print(f\"   Before: {len(df_unified.columns)} columns\")\n",
    "    \n",
    "    stress_level_stats = {\n",
    "        'total_values': df_unified['stress_level'].notna().sum(),\n",
    "        'null_values': df_unified['stress_level'].isnull().sum(),\n",
    "        'unique_types': df_unified['stress_level'].dropna().apply(type).unique(),\n",
    "        'unique_values_sample': df_unified['stress_level'].dropna().unique()[:10].tolist()\n",
    "    }\n",
    "    \n",
    "    print(f\"   Statistics before removal:\")\n",
    "    print(f\"      Total non-null values: {stress_level_stats['total_values']:,}\")\n",
    "    print(f\"      Null values: {stress_level_stats['null_values']:,}\")\n",
    "    print(f\"      Unique types: {[str(t) for t in stress_level_stats['unique_types']]}\")\n",
    "    \n",
    "  \n",
    "    df_unified = df_unified.drop(columns=['stress_level'])\n",
    "    \n",
    "    print(f\"   After: {len(df_unified.columns)} columns\")\n",
    "    print(f\"   Column 'stress_level' removed successfully\")\n",
    "else:\n",
    "    print(f\"\\n    Column 'stress_level' not found (may have been removed already)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d817a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Verification:\n",
      "   No mixed types detected - Data types are now consistent!\n",
      "   All validation checks should now pass\n"
     ]
    }
   ],
   "source": [
    "# Verify fix\n",
    "print(f\"\\n5. Verification:\")\n",
    "type_issues_after = []\n",
    "for col in df_unified.columns:\n",
    "    if df_unified[col].dtype == 'object':\n",
    "        non_null_values = df_unified[col].dropna()\n",
    "        if len(non_null_values) > 0:\n",
    "            value_types = non_null_values.apply(type).unique()\n",
    "            if len(value_types) > 1:\n",
    "                type_issues_after.append(col)\n",
    "\n",
    "if len(type_issues_after) == 0:\n",
    "    print(f\"   No mixed types detected - Data types are now consistent!\")\n",
    "    print(f\"   All validation checks should now pass\")\n",
    "else:\n",
    "    print(f\"   Still found mixed types in: {type_issues_after}\")\n",
    "    print(f\"   Further investigation needed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d7ae30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_after_fix = {\n",
    "    'Row Count': len(df_unified) == 4474,\n",
    "    'Critical Variables Complete': all(df_unified[var].notna().all() for var in ['age', 'gender', 'sleep_quality_norm', 'stress_level_norm'] if var in df_unified.columns),\n",
    "    'Target Variable Present': 'stress_level_norm' in df_unified.columns,\n",
    "    'Data Types Consistent': len(type_issues_after) == 0,\n",
    "    'No Empty Columns': not any(df_unified[col].isnull().all() for col in df_unified.columns),\n",
    "    'stress_level_removed': 'stress_level' not in df_unified.columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1e3d1ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Validation Results:\n",
      "----------------------------------------------------------------------\n",
      "    PASSED     : Row Count\n",
      "    PASSED     : Critical Variables Complete\n",
      "    PASSED     : Target Variable Present\n",
      "    PASSED     : Data Types Consistent\n",
      "    PASSED     : No Empty Columns\n",
      "    PASSED     : stress_level_removed\n",
      "\n",
      "======================================================================\n",
      " ALL VALIDATION CHECKS PASSED!\n",
      "   Dataset is ready for modeling\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUpdated Validation Results:\")\n",
    "print(\"-\" * 70)\n",
    "for check, passed in validation_after_fix.items():\n",
    "    status = \" PASSED\" if passed else \" FAILED\"\n",
    "    print(f\"   {status:12s}: {check}\")\n",
    "\n",
    "all_passed = all(validation_after_fix.values())\n",
    "print(f\"\\n{'='*70}\")\n",
    "if all_passed:\n",
    "    print(\" ALL VALIDATION CHECKS PASSED!\")\n",
    "    print(\"   Dataset is ready for modeling\")\n",
    "else:\n",
    "    print(\" Some validation checks still failing\")\n",
    "    print(\"   Review and fix remaining issues\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "69c86326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Dataset Summary:\n",
      "   Total rows: 4,474\n",
      "   Total columns: 37\n",
      "   Complete features: 4\n",
      "   Target variable: stress_level_norm\n"
     ]
    }
   ],
   "source": [
    "# Update dataset info\n",
    "print(f\"\\nFinal Dataset Summary:\")\n",
    "print(f\"   Total rows: {len(df_unified):,}\")\n",
    "print(f\"   Total columns: {len(df_unified.columns)}\")\n",
    "print(f\"   Complete features: {len([col for col in df_unified.columns if df_unified[col].notna().all() and col != 'dataset_source'])}\")\n",
    "print(f\"   Target variable: {'stress_level_norm' if 'stress_level_norm' in df_unified.columns else 'NOT FOUND'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff530d",
   "metadata": {},
   "source": [
    "## 12. Save Final Unified Dataset\n",
    "\n",
    "### Final Dataset Status\n",
    "- ‚úÖ All corrections applied\n",
    "- ‚úÖ All validations passed\n",
    "- ‚úÖ Data types consistent\n",
    "- ‚úÖ Ready for machine learning\n",
    "\n",
    "### What to Save\n",
    "1. **unified_dataset.csv**: The final cleaned and validated dataset\n",
    "2. **unified_dataset_summary.txt**: Complete documentation of the dataset\n",
    "3. **Updated validation_report.json**: Including the final fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "71b0fcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING FINAL UNIFIED DATASET\n",
      "======================================================================\n",
      "\n",
      "1. Saving dataset to CSV...\n",
      "   Dataset saved successfully!\n",
      "   Path: c:\\Users\\rafae\\Desktop\\Personal_Information_App\\ai_personal_performance_coach\\datasets\\final\\01_unified_dataset.csv\n",
      "   Shape: 4,474 rows √ó 37 columns\n",
      "   Size: 0.51 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SAVING FINAL UNIFIED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save the unified dataset to CSV\n",
    "output_path = FINAL_DATA_DIR / '01_unified_dataset.csv'\n",
    "\n",
    "print(f\"\\n1. Saving dataset to CSV...\")\n",
    "df_unified.to_csv(output_path, index=False)\n",
    "\n",
    "# Get file size\n",
    "file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"   Dataset saved successfully!\")\n",
    "print(f\"   Path: {output_path}\")\n",
    "print(f\"   Shape: {df_unified.shape[0]:,} rows √ó {df_unified.shape[1]} columns\")\n",
    "print(f\"   Size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# Create comprehensive summary document\n",
    "summary_path = FINAL_DATA_DIR / 'unified_dataset_summary.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "21aa66b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Creating comprehensive summary document...\n",
      "    Summary document saved!\n",
      "   Path: c:\\Users\\rafae\\Desktop\\Personal_Information_App\\ai_personal_performance_coach\\datasets\\final\\unified_dataset_summary.txt\n"
     ]
    }
   ],
   "source": [
    "summary_path = FINAL_DATA_DIR / 'unified_dataset_summary.txt'\n",
    "\n",
    "print(f\"\\n2. Creating comprehensive summary document...\")\n",
    "\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"UNIFIED DATASET - FINAL SUMMARY\\n\")\n",
    "    f.write(\"AI Personal Performance Coach - Main Model Dataset\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Creation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"DATASET OVERVIEW\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    f.write(f\"Total Records: {len(df_unified):,}\\n\")\n",
    "    f.write(f\"Total Features: {len(df_unified.columns)}\\n\")\n",
    "    f.write(f\"Total Cells: {len(df_unified) * len(df_unified.columns):,}\\n\")\n",
    "    f.write(f\"Missing Values: {df_unified.isnull().sum().sum():,}\\n\")\n",
    "    f.write(f\"Missing Percentage: {(df_unified.isnull().sum().sum() / (len(df_unified) * len(df_unified.columns))) * 100:.2f}%\\n\\n\")\n",
    "    \n",
    "    f.write(\"SOURCE DATASETS\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    f.write(\"This unified dataset combines three source datasets:\\n\\n\")\n",
    "    source_info = {\n",
    "        '01_sleep_health': {\n",
    "            'rows': 374,\n",
    "            'description': 'Sleep Health & Lifestyle - Sleep patterns, physical activity, health metrics'\n",
    "        },\n",
    "        '03_mental_health': {\n",
    "            'rows': 3000,\n",
    "            'description': 'Mental Health & Lifestyle Habits - Work-life balance, general lifestyle factors'\n",
    "        },\n",
    "        '04_stress_level': {\n",
    "            'rows': 1100,\n",
    "            'description': 'Stress Level Dataset - Mental health indicators, psychological factors'\n",
    "        }   \n",
    "    }\n",
    "    for source, info in source_info.items():\n",
    "        count = len(df_unified[df_unified['dataset_source'] == source])\n",
    "        pct = (count / len(df_unified)) * 100\n",
    "        f.write(f\"  {source}:\\n\")\n",
    "        f.write(f\"    Rows: {count:,} ({pct:.2f}%)\\n\")\n",
    "        f.write(f\"    Description: {info['description']}\\n\\n\")\n",
    "    \n",
    "    f.write(\"TARGET VARIABLE\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    if 'stress_level_norm' in df_unified.columns:\n",
    "        target_dist = df_unified['stress_level_norm'].value_counts().sort_index()\n",
    "        f.write(\"Variable: stress_level_norm\\n\")\n",
    "        f.write(\"Type: Categorical (0 = Low, 1 = Medium, 2 = High)\\n\")\n",
    "        f.write(\"Distribution:\\n\")\n",
    "        for val, count in target_dist.items():\n",
    "            pct = (count / len(df_unified)) * 100\n",
    "            label = {0: \"Low\", 1: \"Medium\", 2: \"High\"}.get(val, \"Unknown\")\n",
    "            f.write(f\"  {val} ({label:6s}): {count:5,} ({pct:5.2f}%)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    else:\n",
    "        f.write(\"ERROR: Target variable not found!\\n\\n\")\n",
    "    \n",
    "        f.write(\"COMPLETE FEATURES (Ready for ML)\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    complete_features = [col for col in df_unified.columns \n",
    "                        if df_unified[col].notna().all() \n",
    "                        and col != 'dataset_source']\n",
    "    f.write(f\"Total: {len(complete_features)} features with no missing values\\n\\n\")\n",
    "    for i, feat in enumerate(sorted(complete_features), 1):\n",
    "        dtype = str(df_unified[feat].dtype)\n",
    "        f.write(f\"  {i:2d}. {feat:35s} [{dtype}]\\n\")\n",
    "    \n",
    "    f.write(f\"\\nPARTIAL FEATURES (Require Handling)\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    partial_features = [col for col in df_unified.columns \n",
    "                       if df_unified[col].isnull().any() \n",
    "                       and df_unified[col].notna().any()\n",
    "                       and col != 'dataset_source']\n",
    "    f.write(f\"Total: {len(partial_features)} features with missing values\\n\\n\")\n",
    "\n",
    "    missing_analysis = []\n",
    "    for feat in partial_features:\n",
    "        missing_pct = (df_unified[feat].isnull().sum() / len(df_unified)) * 100\n",
    "        missing_analysis.append((feat, missing_pct))\n",
    "    missing_analysis.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (feat, pct) in enumerate(missing_analysis, 1):\n",
    "        dtype = str(df_unified[feat].dtype)\n",
    "        f.write(f\"  {i:2d}. {feat:35s} [{dtype:10s}] {pct:5.2f}% missing\\n\")\n",
    "    \n",
    "    f.write(f\"\\nMETADATA COLUMNS\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    metadata_cols = ['dataset_source']\n",
    "    for col in metadata_cols:\n",
    "        if col in df_unified.columns:\n",
    "            f.write(f\"  ‚Ä¢ {col}: Identifies the source dataset for each record\\n\")\n",
    "    \n",
    "    f.write(f\"\\nDATA TRANSFORMATIONS APPLIED\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    f.write(\"1. Variable Normalization:\\n\")\n",
    "    f.write(\"   ‚Ä¢ Age: Normalized to consistent format across all datasets\\n\")\n",
    "    f.write(\"   ‚Ä¢ Gender: Standardized to 'Male', 'Female', 'Other'\\n\")\n",
    "    f.write(\"   ‚Ä¢ Sleep Quality: Normalized to 0-1 scale (sleep_quality_norm)\\n\")\n",
    "    f.write(\"   ‚Ä¢ Stress Level: Normalized to 0-2 scale (stress_level_norm)\\n\\n\")\n",
    "    \n",
    "    f.write(\"2. Feature Engineering:\\n\")\n",
    "    f.write(\"   ‚Ä¢ Created normalized versions of key variables\\n\")\n",
    "    f.write(\"   ‚Ä¢ Added dataset_source identifier\\n\")\n",
    "    f.write(\"   ‚Ä¢ Preserved original features where conceptually different\\n\\n\")\n",
    "    \n",
    "    f.write(\"3. Data Cleaning:\\n\")\n",
    "    f.write(\"   ‚Ä¢ Removed redundant columns (stress_level_numeric, sleep_hours_original)\\n\")\n",
    "    f.write(\"   ‚Ä¢ Removed duplicate normalized columns (sleep_quality from Dataset 04)\\n\")\n",
    "    f.write(\"   ‚Ä¢ Fixed data type inconsistencies (removed stress_level with mixed types)\\n\")\n",
    "    f.write(\"   ‚Ä¢ Converted age to consistent int64 type\\n\\n\")\n",
    "    \n",
    "    f.write(\"4. Merge Strategy:\\n\")\n",
    "    f.write(\"   ‚Ä¢ Vertical concatenation (preserves all rows)\\n\")\n",
    "    f.write(\"   ‚Ä¢ Missing values expected in dataset-specific columns\\n\")\n",
    "    f.write(\"   ‚Ä¢ Common variables normalized before merge\\n\\n\")\n",
    "    \n",
    "    f.write(\"VALIDATION STATUS\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for check, passed in validation_after_fix.items():\n",
    "        status = \"PASSED\" if passed else \"FAILED\"\n",
    "        f.write(f\"  {check:30s}: {status}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nRECOMMENDATIONS FOR MODELING\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    f.write(\"1. Feature Selection:\\n\")\n",
    "    f.write(\"   ‚Ä¢ Use complete features for baseline models\\n\")\n",
    "    f.write(\"   ‚Ä¢ Consider imputation strategies for partial features\\n\")\n",
    "    f.write(\"   ‚Ä¢ Exclude 'dataset_source' from model features (metadata only)\\n\\n\")\n",
    "    \n",
    "    f.write(\"2. Missing Value Handling:\\n\")\n",
    "    f.write(\"   ‚Ä¢ Complete features: No action needed\\n\")\n",
    "    f.write(\"   ‚Ä¢ Partial features: Apply imputation (mean/median/mode) or ML-based imputation\\n\")\n",
    "    f.write(\"   ‚Ä¢ Consider creating missing value indicator variables\\n\\n\")\n",
    "    \n",
    "    f.write(\"3. Data Preprocessing:\\n\")\n",
    "    f.write(\"   ‚Ä¢ Scale numeric features (StandardScaler or MinMaxScaler)\\n\")\n",
    "    f.write(\"   ‚Ä¢ Encode categorical variables (One-Hot Encoding or Label Encoding)\\n\")\n",
    "    f.write(\"   ‚Ä¢ Consider feature selection if dimensionality is an issue\\n\\n\")\n",
    "    \n",
    "    f.write(\"4. Model Considerations:\\n\")\n",
    "    f.write(\"   ‚Ä¢ Target is well-balanced (32-35% per class)\\n\")\n",
    "    f.write(\"   ‚Ä¢ Can use standard classification metrics (accuracy, precision, recall, F1)\\n\")\n",
    "    f.write(\"   ‚Ä¢ Consider stratified train/test split\\n\")\n",
    "    f.write(\"   ‚Ä¢ Handle class imbalance if it appears in subsets\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"END OF SUMMARY\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"    Summary document saved!\")\n",
    "print(f\"   Path: {summary_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24ba6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Creating column reference file...\n",
      "   Column reference saved!\n",
      "   Path: c:\\Users\\rafae\\Desktop\\Personal_Information_App\\ai_personal_performance_coach\\datasets\\final\\unified_dataset_columns.txt\n",
      "\n",
      "======================================================================\n",
      "DATASET SAVED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Files created:\n",
      "  1. 01_unified_dataset.csv (0.51 MB)\n",
      "  2. unified_dataset_summary.txt\n",
      "  3. unified_dataset_columns.txt\n",
      "\n",
      " Dataset is ready for machine learning pipeline!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "columns_path = FINAL_DATA_DIR / 'unified_dataset_columns.txt'\n",
    "\n",
    "print(f\"\\n3. Creating column reference file...\")\n",
    "\n",
    "with open(columns_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"UNIFIED DATASET - COLUMN REFERENCE\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Total Columns: {len(df_unified.columns)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"ALL COLUMNS WITH DATA TYPES\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for i, col in enumerate(sorted(df_unified.columns), 1):\n",
    "        dtype = str(df_unified[col].dtype)\n",
    "        null_count = df_unified[col].isnull().sum()\n",
    "        null_pct = (null_count / len(df_unified)) * 100\n",
    "        status = \"Complete\" if null_count == 0 else f\"{null_pct:.1f}% missing\"\n",
    "        f.write(f\"{i:2d}. {col:35s} [{dtype:10s}] - {status}\\n\")\n",
    "\n",
    "print(f\"   Column reference saved!\")\n",
    "print(f\"   Path: {columns_path}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATASET SAVED SUCCESSFULLY!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  1. {output_path.name} ({file_size_mb:.2f} MB)\")\n",
    "print(f\"  2. {summary_path.name}\")\n",
    "print(f\"  3. {columns_path.name}\")\n",
    "print(f\"\\n Dataset is ready for machine learning pipeline!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ed1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
